module Wizard.Database.Migration.Production.Migration_0007_bookReference.Data.BookReferences where

bookReferences =
  [
    [ "bvq"
    , "7.4.2"
    , "# Submit to an existing database?\n\n## What's up?\n\nIn some cases it might be wise to add your data to existing collections, such as international archives. More and more funding organisations and publishers will actually request that. This is in itself a reasonably straightforward process, as such public archives are usually maintained by dedicated institutes or consortia and these have rules, regulations, requested formats and in many cases clear instructions in how to upload and retrieve your data. Please note that also here, some environments with be 'inert' non-interoperable archive (or only meant to serve human re-use) and some may qualify as HPR environments, which may pose quite some different challenges for you as a data creator (see also earlier consideration on updates and versioning of growing or changing data sets or resources).\n\nA more detailed consideration may be whether you consider your data 'reference' data that may be offered for curation and including the addition to core data resources such as UniProt. In that case the procedures may be quite different and an active interaction with the core data resource custodians is in many cases the way to go. Currently, many of these resources have to painstakingly recover the data they want to use in their curated and value-added resource by *ocular extraction* (reading) or by text and data mining, both of which is cumbersome and error prone. The *direct addition* of your data in the proper, unambiguous format to these core resources is part of good data stewardship practice.\n\n## Do\n\n- Always consider the 'potential reference value' of your data (for instance, can my new findings enrich reference data sources such as Chembl, UniProt or Earthcube).\n- Submit (parts of) your data to the appropriate 'archives' but also provide selected parts of your data to reference core databases whenever appropriate\n- Use the correct formats and standards required by those data resources and if needed contact them.\n- (see resources, databases etc, by category in website?)\n\n## Don't\n\n- Just publish your data as 'supplementary files' with your article(s) and assume that custodians of archives, HPR environments and core reference data  bases will find and use them independently\n- Upload data to such resources without proper metadata and provenance attached.\n"
    ]
  ,
    [ "gwf"
    , "6.5.3"
    , "# How will you document your interpretation steps?\n\n## What's up?\n\nHere we can be short. All issues pertaining to documentation and provenance of the the earlier phases (data capture and processing) also pertain to this phase in the data stewardship cycle. The only thing that may be different is that during this phase it is unavoidable that human input, and therefore error and bias will enter the game. It is therefore the task of a good data stewards to carefully follow (and watch) the interpretation process and warn the team if variation in human interpretation of the actual data will cause aberrations, especially if the interpretation steps involved data curation, selection of subsets of data and reading of unstructured text. Again, recording as much as possible what happens is key to good data stewardship.\n\n## Do\n\n- Act as a 'methodological watchdog' during the data analytics and interpretation process and record 'everything'\n- Make sure that all players in the interpretation process are fully aware of the 'critical points' in the evaluation of the results and the data where human bias and error (that you can not control at the data and the machine workflow level).\n- Encourage everyone to record very carefully what happened exactly at these points, especially when the data are 'curated' or 'turned into another format'\n\n## Don't\n\n- Assume that people are as 'rigorously reproducible' as machines and workflows (in the ideal situation)\n- Interfere with the interpretation process itself unless there is a clear misunderstanding of (the nature of) the data.\n- Step out of the interpretation process altogether, as you will be unable to 'track back' major errors that led to misinterpretation and irreproducibility of the conclusions.\n"
    ]
  ,
    [ "hfj"
    , "6.5.2"
    , "# How are you going to interpret your data?\n\n## What's up?\n\nData interpretation (leading to domain specific scientific hypotheses, conclusions and applications) is not a core expertise of a data steward. However, basic knowledge of the domains relevant for the interpretation of the data is critical. A basic understanding of what the 'domain experts' (increasingly teams from many domains) is crucial for a good data steward. The format, and the needs for 'immediate availability' status of data may be seriously influenced by the choices of the research team and especially the way in which they want to go about interpretation. If interpretation should be mainly done by 'human reading' the requirements for your data output and format will obviously be very different from massive graph traversing to reveal connectivity patterns, followed by conformational reading in externally linked literature and databases. Not only that, when interim results come that clearly indicate major aberrations that are 'simply impossible', you need to be able to spot these before the next working meeting of the entire group reveal them.\n\n## Do\n\n- Consider the anticipated methods for data interpretation very early on (step 1) as they will influence many other steps in data capture, preprocessing, curation and functional integration\n- Make sure you have a basic understanding of the domain knowledge 'around the table' in the research team.\n- Check all 'intermediate outputs' in the process of data processing, linking and analysis for 'obvious' gross anomalies that may indicate errors in parameter setting etc.\n- Carefully record all steps leading to and followed in data interpretation procedures (including where human error and bias may come in)\n\n## Don't\n\n- Act as if 'you only come in when the data are there, process them nicely and deliver beautiful graphs and pictures.\n- Try to become a domain expert; Leave the final interpretation of the results to the experts in the team. Especially when there are multiple domains involved being expert in all these domains is impossible and trying it anyway may be counterproductive as well as irritate the real experts.\n"
    ]
  ,
    [ "quc"
    , "1.3"
    , "# Will you use Reference Data?\n\nReference data are defined as OPEDAS that have a status as a 'reference' data set and can be used to 'interpret' other data. Reference data can be available in many formats and could include curated resources like LITMED in the humanities, UniProt or PDB in the life sciences but also resources like a '5 year twitter trend', a 'gold standard' a for instance a \"chuman reference genome\" that you use to define how your data fits in the larger picture.\n\n## What's up?\n\nReference data sets become more and more important when relatively large new data sets are generated and need interpretation, but also for small data such as for instance a clinical genetics sample with hundreds of variants to be checked against what is known about their phenotypic associations. In some cases (like in ELIXIR) these resources may be branded as 'core resources'. These typically are dynamically updated resources, so they can not be expected to be identical at different times of download or consulting. Increasingly also pre-analysed or pre-linked data sources will enter the realm of core resources and reference data. When these are not provided by authorised providers, with a clear versioning and release policy, it is important to double-check the provenance and the abilities to trace back to the original resources as well as the transparency of the methods used.\n\n## Do\n\n- Make sure you understand the full range of reference data sources available that have potential relevance for your study.\n- Check the 'authority' of these databases and be sufficiently critical about their validity.\n- Make sure the conditions of use suit your purpose (some of these, like HPA, formally forbid for instance commercial use)\n- Ensure that at a later stage you will be able to reproduce the data reused in your study as exactly as possible, and at a very minimum record the version of the reference data resource you used.\n- If you have doubts about reproducibility issues, consider to download the data you cite and archive them properly.\n- Look for existing workflows or methods to (semi-) automatically check your data against reference data to avoid building workflows that already exist.\n\n\n## Don't\n\n- Restrict your search for reference data to the obvious discipline specific resources.\n- Assume that all reference data (including peer reviewed literature) is correct.\n- Use non-authorised reference data if authorised, public reference data is available\n- Use reference data without proper citation or under other people's accounts, as it is important for reference resources to track their user statistics properly.\n- Refer to reference data sources that are updated without a version number.\n- Cite data resources or services that are obviously updated, meaning that people trying to reproduce your results will be led to different versions.\n\n\n## Links\n\n- [DS Question GitHub resources repository: quc](https://github.com/DSQResources/DSQ-quc)\n"
    ]
  ,
    [ "grn"
    , "6.4"
    , "# How will you make sure the analysis is best suited to answer your biological question?\n\n## What's up?\n\nChoosing the best analytical approaches and tools is largely outside the scope of this book as it is the essence of data analytics itself. However, there is a data stewardship edge to this question as well. First of all, it is obviously very important to discuss in step 1 with the best statistician you have access to the nature and the (sample) size of the data you need to answer the question. That is as far as the newly generated data for the experiment at hand is concerned. What is a very important data stewardship aspect here is that in data driven research, very frequently OPEDAS or multiple novel data sets need to be collectively analysed to get the sought-for answers. So, next to the 'data-intrinsic' aspects of size, scope and quality of your data sets and the experimental versus control aspects, there is a serious challenge in functional integration or linking with other data. The size, format, richness (also of metadata) and data model chosen for the capture and storage of your data may be influenced by the other data that are needed to get to a final interpretation of the patterns and phenomena you study.\n\n## Do\n\n- Consider the 'functional integration and linking needs' of newly generated data with the team in step 1.\n- Choose wherever possible the same data formats, metadata standards, vocabularies and data models for your new data as those that were used for the data with which your data should interact.\n- Consider and provide for the mapping, parsing and transformation jobs that may be needed to put other data in a format that can makes them functionally linked and interoperable with your data. An important aspect of FAIR data is that machines should wherever possible be able to parse or transform otherwise any data into any other suitable format (requested by a particular tool for instance) with zero or minimal errors.\n\n## Don't\n\n- Consider data as a one-off asset for a single experiment (most data will be reused inside or outside their original scope/study.\n- Limit your meta data to the conceived need for the study the data were generated for but collect as many as achievable and potentially meaningful\n"
    ]
  ,
    [ "hjq"
    , "6.3"
    , "# Will you be building Kinetic Models?\n\n## What's up?\n\nIn kinetic models the spatio-temporal elements of the phenomenon under study are critical. This means that data to be used in such models need to have the richest possible time (interval) and space coordinates attached as possible. More so than in data that will be used only for static hypothesis, pathway or ontological model purposes, data to be fed to kinetic models must be adorned with rich provenance and metadata about space and time aspects. Obviously, there might be many data sets that were not originally meant to feed kinetic models but can be used for those later on. It is therefore wise to consider all spatio-temporal parameters to be of potential importance for reuse of your data, regardless of whether they were purposely created for kinetic modelling. It should be clear that 'models' as such are also research objects, so all FAIR principles pertain to them as well the rule that you should never make a new model if there are existing ones available that suit your purpose\n\n## Do\n\n- Always ask the research team whether kinetic modelling will be an option in the data analytics steps of the study.\n- Make it a habit to collect and attach 'richer metadata than you would imagine necessary' as not using them will be an option but not having them might be irreparable later.\n- Take extra care of maximum capture op parameters regarding space, time (-interval), volume, level etc. in case data are purposely generated to function in kinetic modelling studies.\n\n## Don't\n\n- Lightly assume your data will never be used for kinetic modelling\n- Fail to discuss the options that this might happen with your team\n- underestimate the 'granularity' of the needed time, space and level details to be recorded.\n"
    ]
  ,
    [ "wgj"
    , "6.2"
    , "# Machine Learning?\n\n## What's up?\n\nIn case your analysis will be (partly) based on machine learning, you need to be aware that machine learning algorithms principally 'build their own model' based on 'exemplar' inputs and exploits algorithms that 'learn' from those examples and then exploit the 'learned' models to predict other outputs and for instance discern similar patterns in novel data. These algorithms are very different from those that 'know the data type they are running over' and therefore are strictly programmed to follow particular instructions reproducibly. Machine learning is usually employed in computing tasks where designing and programming explicit algorithms is infeasible. This obviously has impact on the required quality of the data. Intuitively, you may think that the 'machine will figure out' what it is looking at, and that the FAIR principles do not so much apply to the type of data that are used as input for machine learning. Granted, the data elements themselves may be 'unstructured' (for instance a x-ray image) and thus intrinsically not FAIR. However, in this case the quality and FAIR elements of the annotations as part of the metadata may be even more critical than in cases where the data elements themselves are directly machine actionable and 'understandable' by the machine. For instance, if an algorithm is designed to learn from patterns and colours in images, proper context of in what category the image should be placed is extremely important and can be part of its machine actionable metadata. So again, clearly distinguish between the three categories of data defined earlier: Intrinsic metadata, user defined metadata and the data elements (including the 'raw' data themselves. Machine learning may take place both at the metadata and at the data element level.\n\n## Do\n\n- Distinguish the metadata clearly from the data themselves (machine learning may take place on both.\n- Make sure it is easy for analytics workflows to either 'include' or 'exclude' metadata and annotations.\n- Clearly indicate in the metadata what file type and data model the actual data is in and how it can be accessed for machine learning purposes\n\n## Don't\n\n- Put the metadata and the data elements in the same file without clear distinction (machine learning should be possible on both or separately).\n- Separate the actual data from its metadata to such an extent that the learning algorithm may find the metadata but has great difficulties without intervention to run on the 'raw' data they describe. Even if you consider the raw data not FAIR as in 'immediately machine actionable' they may be actionable in the specific case of this algorithm (from text mining to any other pattern recognition algorithm that works on unstructured data)\n"
    ]
  ,
    [ "ykv"
    , "6.1"
    , "# Will you use static or dynamic (systems) models?\n\n## What's up?\n\nEach set of raw data, but also more processed (relational) data and information is captured in some sort of data 'model'. We distinguish data 'model' from data 'format' as the format is really just a 'form' in which data are presented, such as CSV, Matrix, RDF etc. However, it is very important for a data steward to carefully choose a data 'model' in the sense that the data are presented in the most meaningful and easily understandable (and thus re-usable) manner and in particular the model should be machine readable and actionable in nature. This goes beyond the formatting and identifier related consistency of the data themselves. The fact that we handle this issue under the interpretation and analysis section indicates that this has everything to do with'what the data will be used for. It might be that for a particular study data have to be re-modelled (including in some cases changing them to another data format, or for example changing the units of measurement. This again emphasises the enormous importance of proper annotation (which units of measurement were used etc.) and provenance of the data set. Future users of the data may use them for very different purposes than those that we the reason to create them in the first place.\n\n## Do\n\n- Carefully choose the most appropriate data model for your data\n- Describe data model choice, units of measurement, vocabularies used etc. in great detail in the metadata.\n- Include in the metadata issues that may relate to conversion of (elements of) the data into other models of formats.\n- Carefully study the data (especially OPEDAS) before assuming that you can indeed use them for integrated analysis with other data.\n\n## Don't\n\n- Ever create a new data model or format before carefully checking that no suitable format exists.\n- Assume that the nature (and format choices) of elements of your data (such as units of measurements) are 'self-evident' so that you do not have to specify them.\n- Integrate data for analysis without careful consideration of the compatibility and interoperability of the different data models that 'enter your analysis workflow'.\n"
    ]
  ,
    [ "xvp"
    , "7.5.1"
    , "# Data?\n\n## What's up?\n\nEven if it is impossible to publish your 'negative results' in a classical journal, the data supporting your conclusions (even if an experiment is not reproducible or you claim that earlier results are wrong) can always be published in their own right, accompanied by minimal text to explain what the issue is. It is crucial that the data you publish this way is FAIR as machines should also be aware of controversies, negative results and other 'data warnings'. Linking negative or controversial results to the original papers (DOI's) is important as it allows certain services to automatically annotate these older papers with 'second thought' remarks, even in PDF's (see for instance UTOPIAdocs)\n\n## Do\n\n- Publish all solid data and results, regardless of whether they are considered 'positive' in the sense of generating new insights. In fact the knowledge that 'something is not true or does not work' is also a relevant new insight.\n- Publish negative or controversial assertions as machine readable nanopublications and push them for peer review\n\n## Don't\n\n- Ignore negative results unless you can explain why the results do not represent valid new associations or insights for instance because review of the experimental set up or the study reveals fundamental flaws. Even in that case the reasons for the flawed set up may be worth publishing because it may prevent others from makign the same mistake.\n- Try to publish negative results in classical narrative articles/journals, especially not in pre-peer review journals it will only yield frustration.\n"
    ]
  ,
    [ "hct"
    , "1.12.1"
    , "# What will the IP be like?\n\n## What's up?\n\nFor new data you created, the IP is a priori with 'you' or your organisation, unless otherwise specified by the funding body supporting your research. For data on human subjects however, the legal ownership of the data is most likely with the individuals that participated in the study. In that case you are a 'custodian' of other people's data by default. This brings many additional responsibilities as a data steward. Obviously, considering how to deal with that IP is important, also for ad hoc experimental data. However, for the creation of reference data, and specifically when it means the extraction, transformation, reloading, curation and 're-packaging' of OPEDAS to create a reference data set, the IP situations and the license under which these source data can be reused are extremely important. The legislation in such accumulated databases (including their implicit ownership) can be different in different countries and regions. The licenses of the source data may 'carry over' to you new dataset and including elements with highly restrictive licenses (for instance restricted to non-commercial use only) may later jeopardise your attempts to make the reference data set widely used and your abilities to 'exploit' the reference data for sustainability purposes.\n\n## Do\n\n- Check thoroughly the IP situation and the licenses of all data sources you intend to use for inclusion\n- Also if you only include small subsets of larger resources, even if they are 'public' at first sight and not password protected (there might still be restrictions)\n- Check whether the sources you want to include have properly dealt with the IP situation of sources they included\n- Contact data source owners when you have doubts about the licensing or IP situation of that resource\n- Consult legal and licensing experts to ensure that you do not violate any explicit or implicit rules by creating the intended reference data set.\n\n## Don't\n\n- Create reference data without FIRST consulting a licensing expert [find Wilbanks' OPF doc]\n- Create reference data without all these check because you think 'only your own department will use them' (that would almost disqualify them as 'reference data' in the first place.\n- Underestimate the time and effort needed and the scrutiny of for instance Pharmaceutical industry to use your reference data (and maybe even pay for it) later. Even data sets without any license (and seemingly open) form a strong liability for large commercial companies to sue in forma approval processes for instance.\n- Choose a license for your own data without professional advice and use the most open license acceptable for your reference data (in Open Science number of users roughly equals chances for sustainability).\n\n## Links\n\n- [DS Question GitHub resources repository: hct](https://github.com/DSQResources/DSQ-hct)\n"
    ]
  ,
    [ "usx"
    , "1.12.2"
    , "# How will you maintain it?\n\n## What's up?\n\nReference data sets can be very important for interpretation, review and reproducibility of other people's experiments and results. Creating reference data is therefore a contribution to the common good of research data and therefore long-term maintenance is a key issue. A frequent problem is that reference data sets are initially set up at a rather small scale, within specialised academic institutes and that sustainability is only addressed towards the end of the research project that generated them. Short-cycle funding systems are generally spoken not sufficient to guarantee long term sustainability of reference (core) resources. It is therefore of great importance to plan the creation and the long term maintenance of reference data in advance and to carefully consider choices that will influence 'maintainability' later on, such as data format, level of automation versus manual curation, data infrastructure choices, number of expected users, network capacity and the ability and or willingness of your institution to maintain 'public resources' beyond the immediate use for internal research purposes. If any of these are 'negative' your resource may actually bring other people that use it into trouble in the long run as part of their scientific discourse relied on your data and non-availability will affect them directly.\n\n## Do\n\n- Check thoroughly with your supervisors and institute whether maintenance of reference data beyond the research project is an option\n- Seek advice from professional data owners (preferably of larger sets than the one you intend to create) to ensure optimal planning of the supporting infrastructure and the curation and maintenance effort. (reuse of your data by others may bring significant correspondence, support effort, but also co-publications with it, discuss this with your supervisor)\n- Specifically discuss a maintenance plan with your group, support staff and supervisors, including a versioning and release plan (for both the data itself and the API's and GUI's if relevant) and a customer-support plan.\n- Consult legal and licensing experts to ensure that offering the data set is 'maintainable' from a legal perspective\n- preferably work with a professional (sometimes commercial) data provider for the 'commodity' part of maintaining the reference data resource, and separate the 'research and innovation' environment very strictly from the 'commodity part' both in terms of funding approach and support.\n\n## Don't\n\n- create (potential) reference data on public money with the intent to close them off for other researchers unless there is a very clear and defendable reason for this (you will increasingly have to argue very strongly for non-open data in your research proposals and keeping data closed may lower your selection chances)\n- Assume that 'someone in the department' will pick up the longer term stewardship of your data because they are 'so obviously valuable'. Many universities have no incentives to structurally 'provide services' to third parties.\n- Create reference data without the utmost care to make both the data elements themselves and the metadata F.A.I.R. and machine actionable wherever possible.\n\n## Links\n\n- [DS Question GitHub resources repository: usx](https://github.com/DSQResources/DSQ-usx)\n"
    ]
  ,
    [ "gqs"
    , "1.15.1"
    , "# What is the volume of the anticipated data set?\n\n## What's up?\n\nVolume is only one of the aspects of your data set (others are complexity, privacy, variety etc.). However the volume itself may bring some formatting considerations already. In some cases you may be able to store only part of the data without the loss of essential information. For instance, if you have done full genome sequencing on a group of individuals, you may be able to store one reference genome (with rich provenance of course) and store on the sequence variations of each individual from that reference genome. This will reduce your stored data set enormously (less than 1% of the original sequence) and at any given moment you may be able to 'regenerate' the entire genome of each individual by 'adding the rest of the sequence form the reference genome file'.\n\n## Do\n\n- Always consider data volumes first and decide how these reflect on choices you make in:\n- Preprocessing choices\n- Provenance storage (very detailed logging of what happened to the original raw data, how, when and why\n- Format of the resolution processed data, based on workflow choice, common practice etc.\n- pre-check that you institutional (or external) storage and compute infrastructure and access is sufficient to properly pre-process , store and access the data.\n- Anticipate as well as possible your own groups' and potential future use of the data (see resources)\n\n## Don't\n\n- Assume without checking that the compute and storage facilities you have access to are adequate.\n- Create any file formats, vocabularies or other data related assets without first convincingly demonstrate."
    ]
  ,
    [ "bqy"
    , "1.9"
    , "# Is re-consent needed?\n\n## What's up?\n\nIn case OPEDAS are subject to informed consent rules, you have to ensure that the consent given covers the specific purpose of your intended study.\n\nThe GDPR in Europe and the [equivalents in other regions] need to be respected. Even if you do not expect it, re-consent of the OPEDAS owner(s) may be a legal prerequisite (even the owners may not be aware of this legal situation) and therefore utmost care has to be taken that you do not violate any laws. It is also advisable to discuss the reuse of OPEDAS with your ethical committee in case it involves data directly derived from studies involving animal or human subjects, or data from for instance 'social media' origin.\n\n## Do\n\n- Check the consensus-statements associated with the data (if any)\n- Contact the data owner actively when there is any doubt about the legal issues associated with the reuse of the data\n- Prepare a clear explanation of why you want to reuse the data and for what purpose\n- Ensure the level of explanation is fit for the owner\n- Double check with the owners for how long they intend or guarantee to keep the data available in the same format and version.\n\n## Don't\n\n- Expect that all OPEDAS owners are prepared to share their data for any research purpose.\n- Underestimate the burden you may put on the OPEDAS owner\n- Ignore other incentives then 'citation', such as for instance a formal acknowledgement or co-authorship\n- Use any potentially sensitive or otherwise restricted data for your experiments without all the checks above, even if they 'seem' to be freely available on the Web.\n- Take for granted that data that can be used for pre-competitive research can also be used for commercial purposes (when you work for a company license and consent issues are even more pressing in some cases than for public research institutions)\n\n## Links\n\n- [DS Question GitHub resources repository: bqy](https://github.com/DSQResources/DSQ-bqy)\n"
    ]
  ,
    [ "bzu"
    , "6.6"
    , "# Will you be doing (automated) knowledge discovery?\n\n## What's up?\n\nIf the interpretation process involved automated querying of reference resources and/or for instance text mining pipelines you should be aware, and make the rest of the team aware' of all issues pertaining to 'using OPEDAS' described earlier. OPEDAS and workflows may be subject to versioning, extension, curation etc., which may heavily influence sequential results of such processes, even if they are 'automated'. Especially when such 'query and interpretation pipelines are used by multiple members of the research team they should all be working with exactly the same version of all tools and resources. This can be achieved much easier if all interpreters are working on standardised machines you provide, but in most cases teams are spread around the world and may use either on-line versions of tools, web services and locally installed versions. If these are not calibrated and aligned in the same scrutinised way as the machines that captured the data in the first place, major errors may be introduced.\n\n## Do\n\n- make the research team fully aware of all error-sources imaginable during the interpretation process\n- be fully aware of each and every person in the team that plays a role in the interpretation process\n- ensure that all of them have EXACTLY the same version of every tool and resource they need for the interpretation process\n- ensure that all 'automated discovery' pipelines that may be started up during the interoperation process are identical.\n- Log every step of the interpretation process and force the domain specialists to record versions of every tool and resource they use.\n- record with each subset of interpretation-related data sets who created that subset.\n\n## Don't\n\n- trust that domain specialists are fully aware of the many pitfalls in tools that to them come across as 'commodities' and 'provided by the data people'.\n- contribute to the messy situation by being sloppy in version control of tools and resources yourself.\n- pool interpretations from different people in the team or from different automated systems before checking the integrity and the versioning of everything.\n\n## Links\n\n- [DS Question GitHub resources repository: bzu](https://github.com/DSQResources/DSQ-bzu)\n"
    ]
  ,
    [ "dcy"
    , "1.8"
    , "# Will owners of that data work with you on this study?\n\n## What's up?\n\nSome OPEDAS may not be usable without explicit consent of the owner and without assistance of the owner. This does not mean the data are necessarily badly documented or of low quality. They may be restricted by privacy laws, connected to a tissue biobank or even personal data of an individual citizen who is the legal owner of a personal data locker. If you need cooperation from a OPEDAS owner, there are additional issues to consider, such as time constraints, co-authorship and informed consent.\n\n## Do\n\n- Check the conditions under which you can get access, and contact the owner\n- Prepare a clear explanation of why you want to reuse the data and for what purpose\n- Ensure the level of explanation is fit for the owner\n- Check verbally and in writing with the owner about potential restrictions that might not be explicit in the metadata of the OPEDAS.\n- Double check with the owners for how long they intend or guarantee to keep the data available in the same format and version.\n\n## Don't\n\n- Expect that all OPEDAS owners are prepared to share their data for any research purpose.\n- Underestimate the burden you may put on the OPEDAS owner\n- Ignore other incentives then 'citation', such as for instance a formal acknowledgement or co-authorship\n- Ever use data you find on the Web or elsewhere without making absolutely sure you are not violating informed consent or other generic rules.\n\n## Links\n\n- [DS Question GitHub resources repository: dcy](https://github.com/DSQResources/DSQ-dcy)\n"
    ]
  ,
    [ "grt"
    , "4.2.2"
    , "# Can workflows be run remotely?\n\n## What's up?\n\nIf you can choose a cloud-based workflow that can be run remotely, or even better a workflow that can be containerized and sent to data that remain '*in situ*' that may be the best choice in most cases. As argued before, data sets increasingly become too large or too sensitive (human data) to be pumped around to different storage and compute locations in the consortium you may be operating in. Sending data around is obviously associated with all kinds of technical and security issues. This consideration will be dealt with in more detail when we discuss data analytics but even for pre-processing and curation of data, moving the data as little as possible and using remote or 'locally downloaded' workflows to deal with these processes is the preferred option.\n\n## Do\n\n- Consider the size of the data set to be processed and or curated (as far as computer-aided curation is concerned) before you decide to work with a stand alone, local workflow or a remote service.\n- Use workflows that are provided as a 'visiting service' in general as more reliable and desirable then services for which you need to send you data physically to the compute.\n\n## Don't\n\n- Send data around to services unless this is the only or the clearly preferred option.\n- Decide on this purely on a data science viewpoint but consult the rest of the team to also consider cost issues, sustainability issues, reproducibility issues and most importantly, ethical and security issues.\n\n## Links\n\n- [DS Question GitHub resources repository: grt](https://github.com/DSQResources/DSQ-grt)\n"
    ]
  ,
    [ "ixr"
    , "2.5.1.2"
    , "# Can the original data be regenerated?\n\n## What's up?\n\nIn some cases it might be cheaper (and acceptable) to regenerate data rather than storing them. Two examples: It may soon become cheaper to 're-sequence' a genome than to store it for 10 years. Also text mining the same massive corpus of text with the same tagger and the same thesaurus, should in principle give the exact same result when repeated at any time. However, in both examples, a number of assumptions would have to be made before a decision would be made to re-generate the data rather than storing the first version for extended period of time. First of all, the technology should not change; sequencers get more reliable by the day and therefore may give different results and the 'old sequencer' may not be in your possession anymore by the time you want to generate the results. Workflows are not necessarily stable but more importantly even 'stable' substrates (a genome of a living individual or a corpus of text) may not be as stable as you think. Changes to a text corpus may occur unbeknownst to you, but also, the somatic mutation rate in the genome of a living organism are not insignificant and therefore a new sample of cells to take DNA from may give different results and even if the DNA sample was stored in 'preserved state' there is no absolute guarantee that later re-sequencing of it will give exactly the same result. So in all cases, the decision to 'regenerate versus store' is a deep-scientific method discussion in the group and not a 'trivial decision'\n\n## Do\n\n- Consider all angles of the problem including deep domain knowledge issues like exemplified above when a decision for regeneration of 'identical' data is to be considered.\n- Only consider this option if the long term preservation is problematic due to size/costs or other aspects.\n- Always keep careful records of whether data are indeed 'exactly the same' (as far as that can ever be guaranteed) or 'supposedly the same but regenerated from the same substrate with the same methods.\n\n## Don't\n\n- Lightly assume that data can be easily regenerated even if that seems to be apparent with only superficial knowledge of the scientific subject.\n- Re-generate data and archive them under the same PID as the original data set, assuming that there will be no differences.\n\n## Links\n\n- [DS Question GitHub resources repository: ixr](https://github.com/DSQResources/DSQ-ixr)\n"
    ]
  ,
    [ "jrw"
    , "4.2.1"
    , "# Who are the customers that use your workflows?\n\n## What's up?\n\nMany workflows will be originally developed by computer or data scientists and scientific programmers. Apart from the first versions usually being professorware (which may still be the best choice you have) they are frequently not very easy to use by the experimental scientists in the group. This means that if you choose for such early development pipelines and work workflows (which again may be your only option in some cases) it might have the embedded consequence that you will have to be stand-by when other members of the group need to run the workflow or in the worst case scenario that all instances and runs have to be performed by yourself. If the choice of workflow means that you become the 'single point of failure' in the chain of events, this is a sub optimal choice by default. So in case this choice is unavoidable, make sure you train as many people as possible to use the workflow as well (which is again different from being able to co-develop on it in terms of new or customised functionalities).\n\n## Do\n\n- Always go for the most 'mature' and user friendly workflows that can be operated by as many people in the research team as possible\n- consider commercial and proprietary solutions as well, but use the same balancing as described for any choice of workflows.\n- Make test runs with different workflows (if available) with the rest of the research team in the loop and collectively choose the best suited option.\n\n## Don't\n\n- Choose a workflow that seems easy to you as a data expert but may be non-operational for other group members.\n- Get carried away by options to 'hock on it yourself' at the expense of user friendliness and stability.\n- Overestimate the ability of your fellow group members to deal with difficult or nerdy interfaces (that seem easy and intuitive to you). Many of your team members will panic when they get a strange computer-generated message on their laptop.\n\n## Links\n\n- [DS Question GitHub resources repository: jrw](https://github.com/DSQResources/DSQ-jrw)\n"
    ]
  ,
    [ "jwg"
    , "2.2.3"
    , "# How will you describe your data format?\n\n## What's up?\n\nFor machines (even more than for people) it is imminently important that the format and the structure of the data you put up for re-use is well defined. In fact, once the data type, the data format and the individual data elements are all 'understandable' for the 'visiting compute', machines will be able to reuse your data and if needed transform them into other desired formats with minimal risk for errors.\n\nIt is therefore very important to include information about these elements (format, structure, terminology systems used) in your 'intrinsic metadata'. Perfectly formatted data with no information on format, structure and terminology system information may still be processed by machines (parsing all elements and 'reconstructing' what each element is all about), but it is not considered good practice to leave machines with much less information than they need and let them figure out what the data might be about. There are several projects focusing on data provenance, extended annotation, retrieval and citation and you should study their recommendations (and work with them and/or their services where appropriate) to make sure that your metadata themselves have a well defined format and they contain rich information on the format of the data themselves.\n\n## Do\n\n- Study established metadata formats.\n- Keep your metadata schema and data formats as simple as possible\n- Use existing libraries and catalogues of data types and formats wherever possible.\n- Record and provide the richest possible metadata format as it is not always easy to anticipate reuse later, but minimally (in the context of this topic) include information on data format, structure, size and terminology systems used to refer to concepts in the (meta-)data.\n\n## Don't\n\n- Use data formats that are not generally accepted by the community (and hence by established workflows) unless unavoidable.\n- Assume that your data are structured and 'FAIR' enough in and off themselves to get away with minimal metadata. FAIR Metadata will increasingly be used by search engines to find and recommend your data, so reuse will be largely dependent on FAIR metadata.\n\n## Links\n\n- [DS Question GitHub resources repository: jwg](https://github.com/DSQResources/DSQ-jwg)\n"
    ]
  ,
    [ "jxb"
    , "1.5"
    , "# What format?\n\n## What's up?\n\nSeveral data sets and in particular reference data resources may be available in different formats. For instance, core resources in the life sciences such as UniProt and HPA increasingly offer their ore data in machine readable (FAIR) formats. You need to be fully aware of the data formats used, the limitations of the format, the possible license restrictions and thus the way in which you may or may not be able to reuse these data. Please be aware that even for data resources that are not password protected in any way and can be freely used at face value for academic purposes there may still be restrictions on use.\n\n## Do\n\n- Check the data formats available at the sites of the OPEDAS sources you have selected to use.\n- Make sure to pick the best suited for the analysis you intend to do in your study.\n- If the desired format is not readily available, consider to contact the resource owner to discuss if the data can be made available in that format.\n\n## Don't\n\n- Use sub-optimal data formats without checking the availability (maybe at other sites) of the correct format.\n- Use data for a particular purpose without making sure the format and the potential restrictions will not jeopardise your analysis or your abilities to publish.\n- Use data or resources that are only for academic use if you intend to use the results later for potential innovation of commercila purposes.\n\n## Links\n\n- [DS Question GitHub resources repository: jxb](https://github.com/DSQResources/DSQ-jxb)\n"
    ]
  ,
    [ "kuz"
    , "1.13"
    , "# Will you be storing physical samples?\n\n## What's up?\n\nLet's define a 'biobank' as any collection that contains (reference) biological samples. Obviously, most biobanks are collected for research purposes. In case the samples are of human original, or associated with ethical and privacy considerations for any other reason, the stewardship aspects are significant. As you may have noticed, the term 'data' was not coupled with the term 'stewardship' here. We can argue that even a tissue sample is a 'form of data' but as it is not digital we will separate the discussion about the physical samples from the data stewardship associated with Biobanks. The Biobanking world is better organised than many other fields and therefore all the issues related to the stewardship of (samples in) biobanks will not be covered here, and can be found under 'resources' of this section. However, it should be emphasized here that the 'annotations' and the metadata of the the samples and the biobank collection as a whole should be F.A.I.R. like any other data.\n\nIt is also clear that in many cases broad metadata about a biobank and the broad collections it contains can be 'open' while the actual details on and the access to the samples themselves is highly controlled and restricted for obvious reasons.\n\n## Do\n\n- Check thoroughly with experts in your institution to find out whether biobanks (and all related infrastructures' already exist in your institution.\n- Seek advice from professional biobank owners (most likely of larger sample and data sets than the one you intend to create) to ensure optimal planning of the supporting infrastructure and the curation and maintenance effort. (reuse of your samples and data by others may bring significant correspondence, support effort, but also co-publications with it, discuss this with your supervisor)\n- Specifically discuss a collection maintenance plan with your group, support staff and supervisors, including a versioning and release plan (for both the samples and the annotations and metadata and a customer-support plan.\n- Consult legal and licensing experts to ensure that offering the samples and data set is 'maintainable' from a legal perspective (see for options to deal with informed consent under 'resources'\n- preferably work with a professional (sometimes commercial) infrastructure and data provider for the 'commodity' part of maintaining the reference samples and data resource, and separate the 'research and innovation' environment very strictly from the 'commodity part' both in terms of funding approach and support.\n- Make sure you have a strong sustainability plan.\n\n## Don't\n\n- create a biobank based on public money with the intent to close it off for other researchers unless there is a very clear and defendable reason for this (you will increasingly have to argue very strongly for non-open collections and especially their metadata data in your research proposals and keeping data closed may lower your selection chances)\n- Assume that 'someone in the department' will pick up the longer term stewardship of your samples and data because they are 'so obviously valuable'. Many universities have no incentives to structurally 'provide services' to third parties.\n- Create reference samples and data without the utmost care to make both the annotation elements themselves and the broader metadata F.A.I.R. and machine actionable wherever possible.\n\n## Links\n\n- [DS Question GitHub resources repository: kuz](https://github.com/DSQResources/DSQ-kuz)\n"
    ]
  ,
    [ "ikk"
    , "2.2"
    , "# Will you be using new types of data?\n\n## What's up?\n\nIt might be the case that the type of data you will create is entirely new. In that case, obviously, standard formats do not come automatically. However, even for entirely new types of data any of the existing data formats or templates may be well suited or the combination of existing formats, vocabularies etc. will suffice. Whenever this is an option, the reuse of existing formats is preferable over the creation of entirely new data formats. There are communities specializing on these data related topics and it is highly advised to work with such communities to solve your data modelling issues.\n\n## Do\n\n- Study existing formats and vocabularies (especially also from other disciplines) for suitability for your data type.\n- Consider combination of existing elements if no standard best practice exists.\n- Consult experts in similar data types to make sure you did not miss anything.\n\n## Don't\n\n- Ever create a new data format, under the assumption that your data is unique without a thorough study of what exists.\n- Create data types without consulting experts in the the field of related data types.\n\n## Links\n\n- [DS Question GitHub resources repository: ikk](https://github.com/DSQResources/DSQ-ikk)\n"
    ]
  ,
    [ "njy"
    , "2.1"
    , "# Are you using data types used by others too?\n\n## What's up?\n\nUnless you do entirely novel types of research, there are likely to be multiple data formats around in which the types of data you generate may be captured, processed and formatted. Some of these may be 'exotic' and not used (anymore) by the majority of the community, which frequently means that they will be difficult to find, map, inter-operate and reuse. In addition, it is less likely that standard workflows will process these data formats. Especially in case the intention to use the data generated in combinatorial or integrated experiments with OPEDAS, the formatting of your data is extremely important. In many cases, data in proprietary or exotic formats can be munged and recreated into more commonly used formats, but these processes are very cumbersome and error-prone. It is therefore of the utmost importance to consult the expert community and get the data in the most optimal formats of further analysis and ultimately for reuse by your own group and others.\n\n## Do\n\n- Always use community-compliant, supported and sustainable data formats whenever possible.\n- Turn to experts to tell you what are the best formats to use for the particular data types you will create.\n- Ensure you are prepared to answer questions on the use of the data (for instance, which workflows will they be subjected to).\n- Choose the formats with the richest expression possibility. It is easier to leave things blank then extending a poor data format later.\n\n## Don't\n\n- Assume that your data is so unique that it needs an entirely new format.\n- Think that a spreadsheet with free text labels or your locally developed database is the best way to store and reuse your data.\n- Format and store data in any format without keeping rich and relevant metadata and provenance.\n- Throw away the original data unless you are absolutely sure that storing them has no further added value, for example for review of experimental and analytical procedures. Not having certain pre-formatted data available may actually preclude the publication, reuse and citation of your (original) data by others and might also jeopardise the publication of accompanying articles.\n\n## Links\n\n- [DS Question GitHub resources repository: njy](https://github.com/DSQResources/DSQ-njy)\n"
    ]
  ,
    [ "hea"
    , "2.1.1"
    , "# What format(s) will you use for the data?\n\n## What's up?\n\nOnce you know the general 'type' of the data you will generate, it is important to check carefully what the best format will be to capture, process and format the data in. In many cases, the capture format will be initially dictated by the software packages coupled to the instrument through which the data are measured or collected (from questionnaires to high throughput machines). However, once the raw data have been collected, you may have many choices in how to pre-process it and as to which format(s) to choose for the final version of the data to go into further analysis. Considerations for choice must be mainly based on FAIR principles and community adoption of the format, which is in turn related to the availability of mapping tools, integration tools and analysis workflows in general. Having your data in 'easy, local custom' formats like Excel with free text in cells may put severe pressure on your resources later as significant effort may have to go into formatting with all negative effects described earlier.\n\n## Do\n\n- Carefully study the data formats available for the type of data you will generate.\n- Study their community adoption rate\n- Check in how far the resulting data will answer to the FAIR principles.\n- Make sure the data format itself and any changes/additions you may have introduced to it are well documented and part of the (FAIR) metadata.\n- Inform the provider of the data format and templates on any prerequisites in the templates or formats that would render the data non machine actionable.\n\n## Don't\n\n- Use free text in any part of the data format without underlying PIDs.\n- Follow instructions in data templates when you can predict this will render data not FAIR (contact the the template owner to correct).\n- Ever design a data template or format unless you are absolutely sure their is no community adopted alternative.\n"
    ]
  ,
    [ "atw"
    , "2.3.2"
    , "# Will you store licenses with the data?\n\n## What's up?\n\nAlways consider the use of your data beyond the original purpose. One of the issues with re-using other people's' data is that they cannot be assumed to be reusable from an ethical or legal standpoint without explicit permission. Assuming that unlicensed data are 'free to use for whatever purpose' is intrinsically wrong, and in the case of for instance Pharmaceutical industry can lead to court cases later on. Therefore, whenever you publish a data set or any other kind of information or digital object, it is important to define a license for reuse. For software many licences exist, and for data, increasingly, standard licenses are available or under development. Please note that a given license is also a defined concept and therefore deserves a Persistent Identifier and a URI pointing to where the license can be studied (machine readable licenses are also under development in some areas). This means that in the metadata, the license under which the data or the workflow can be reused is 'just another PID in the right place'. Users can then specify in their search or workflow container that 'only data with the following licenses should be included'.\n\nFor instance, if you include some data in your analysis that cannot be used for commercial purposes, that decision may render your entire results not usable for commercial purposes (at least in the view of some lawyers). This means that not licensing your data at all, even if you don't care who uses them and for what purpose is very counterproductive and will severely undermine the actual reuse of your data by others and in particular by industry. It will also lower the attribution-rate (usually part of the license conditions) and thus the citation and the impact score of your data.\n\n## Do\n\n- Always carefully choose a license to be attached to your data upon publication.\n- Include and clearly mark the licences PID as a concept + attributes in the metadata.\n- Store and 'expose' the license as part of the metadata in Open Access environments where search engines can easily find the license, even of the data they describe are not (yet) FAIR or even highly restricted in access. The 'fact' that a data set with a specific license is 'out there' is a first step toward effective reuse of your data or information source.\n- Make sure, especially when you restrict use of your data, that you are able to enforce the license you choose. Licenses that are not enforceable make no sense. (please note that the enforcement is usually not done by an individual research group but at institutional or repository level)\n\n## Don't\n\n- Ever publish data without a license attached or choose a license lightly, without considerations of anticipated reuse of your data.\n- Choose a license that is not transitive (i.e can not be transferred with subsets of the data), but make sure its transitivity does not unduly restrict the reuse of your data.\n- Choose an unnecessary complicated license with many clauses and wherever possible one that is already widely adopted in the research community for either software or data.\n- Restrict the reuse of data any more than absolutely necessary: for data generated with public funding the default is usually completely Open, only restricted in any way if there are very good ethical or strategic/commercial reasons.\n- Opt out from Open Data lightly: Most public funding agencies will request open data publishing as a default as part of their funding conditions. Usually there is an 'opt out' option, but DO NOT use that unless unavoidable: With the fierce competition for research grants any element in your grant that can make it less attractive to reviewers (and keeping your data out of the public domain is certainly one of them) may cause rejection, even if it scores in the eligible ranks based on the science case. So the advice is to only restrict the reuse of data (especially for projects funded from public sources) if there is 'no reasonable alternative' and make sure you make a very strong point in your data stewardship plan about the underlying reasons.\n\n## Links\n\n- [DS Question GitHub resources repository: atw](https://github.com/DSQResources/DSQ-atw)\n"
    ]
  ,
    [ "pth"
    , "1.11.1"
    , "# Will you need to add data from literature?\n\n## What's up?\n\nIn case you consider to use (machine readable) data that are directly mined from the literature, you should carefully consider whether you should mine these yourself or whether appropriate machine readable collections of the relevant literature already exist in the machine readable format. Many groups have specialised in text mining and some of them provide the results of their decade long efforts in F.A.I.R. format (examples). Unless you are in a specialised text mining group chances are that it will cost you a prohibitive effort to reach anywhere near the same quality as these specialised groups can offer.\n\n## Do\n\n- Check whether collections of pre-mined data from the literature are available in machine readable/analysable format. [example of semantic medline?]\n- Treat these as OPEDAS (see sections above) and contact the OPEDAS set owner (usually the text mining expert) to check on disambiguation levels, precision and recall and other key features of the data se\n- Ensure that the output data are in the correct format and consider the balance between for instance mapping the data to a different term system versus re-mining them with a different tagger (the latter may actually be easier and faster if you collaborate with the experts.\n\n## Don't\n\n- Think that computers can properly read text, they will give you very ambiguous and noisy results, due to homonym, synonym and syntax problems.\n- Consider to use text-mined results without studying in some depth the many issues associated with conversion of human readable narrative to machine readable data\n- try text-mining yourself with amateur tools, it will almost surely lead you into a time consuming and unproductive side-track, contact experts instead and collaborate\n\n## Links\n\n- [DS Question GitHub resources repository: pth](https://github.com/DSQResources/DSQ-pth)\n"
    ]
  ,
    [ "qzt"
    , "4.1.1"
    , "# Will you be running a bulk/routine workflow?\n\n## What's up?\n\nA data steward is not a software developer. Developing a new workflow for data processing and curation should be avoided where possible. It is highly unlikely that your data (pre-) processing and curation process is entirely new. Obviously, in case the data type is indeed 'new' (the first of its kind instrument, measuring first of its kind data) you will need to develop data processing software that is not co-delivered with the instrument. As said, in that case, follow all recommendations and procedures of software carpentry, realise you are probably producing '*professorware*' in first instance and do not get carried away with the beauty, the publishing and the market potential of your software. But most of your effort should go into research and consultation on existing tools.\n\n## Do\n\n- Check the tool registries of established research consortia or infrastructures in your domain for existing workflows that serve your need.\n- If established registries do not seem to have the tools you need, do a web search to find out less established tools (may still save you a lot of time).\n- Arrange for remote or face to face consultation and training if needed.\n- Test-run workflows on reference data sets to get familiar with the system before you submit your valuable new data to it.\n- Try to establish contact with the supporter(s) of the software and tools you choose and make sure you know the version and release policies and practices as well as the service agreements that are in place.\n\n## Don't\n\n- Assume your data and process is so unique that you need to develop workflows and algorithms from scratch without a thorough (international) search for existing options.\n- Present the software or algorithms you may have to develop as 'solutions' that are usable and scalable for others without going through all the steps required to make professional software (that does not mean others cannot use your 'professorware' solution, but be aware of the time and effort it will take to help others to use your tools without proper documentation and support (you will likely be their default 'help desk').\n- Ever produce 'slightly better' software when good tools are available and their performance is good enough for your purposes.\n\n## Links\n\n- [DS Question GitHub resources repository: qzt](https://github.com/DSQResources/DSQ-qzt)\n"
    ]
  ,
    [ "rbz"
    , "1.12"
    , "# Will reference data be created?\n\n## What's up?\n\nIf you create data that are meant to become a reference data set there are even more challenging details to be aware of. In addition to all data stewardship considerations described for 'normal' data, the creation of reference data sets is very specifically directed at intended reuse. Very much like with software, when other people try to use your code, lousy documentation is very bad practice, and causing other people a lot of trouble. Please also note that creating reference data is not necessarily an act of experimental data creation. It is very well possible (if not the rule) that reference data are created by collection, reformatting, integration, curation and annotation of existing data. After these cumbersome data munging processes, the resulting set is then offered for reuse by others. Some of such data bases (like UniProt) became so-called core resources (almost everyone in the domain uses them and will reference them in new studies. So, citability, professional versioning of the data itself and for instance API's becomes even more critical than with individual new datasets.\n\n## Do\n\n- Check thoroughly whether reference data already exist (creating new reference data is very expensive and time consuming)\n- Check what the predominant existing community formats and standards are for the data type(s) you intend to create.\n- Make a list of term systems to be used for the concepts to be referred to in your database\n- Give construction of the data infrastructure, performance of your ICT infrastructure and your ability to maintain and update the data (if needed) sufficient attention\n- consider early on to work with larger and professional data centers that can set up, maintain and support the reference data\n- Carefully budget for the creation and the maintenance of your reference data in your research plan.\n\n## Don't\n\n- Create reference data without a very specific scientific purpose and need\n- Create redundant reference data\n- Host reference data in your own academic research environment/infrastructure if at all possible\n- Assume that 'others will take over' your reference data and maintain it properly without checking\n- Create reference data without a proper release, update, API and versioning policy plan.\n- Create data first and then realise they could or should be reference data and now you did not follow the correct procedures.\n\n## Links\n\n- [DS Question GitHub resources repository: rbz](https://github.com/DSQResources/DSQ-rbz)\n"
    ]
  ,
    [ "rgy"
    , "1.6"
    , "# Is the data resource versioned?\n\n## What's up?\n\nEspecially core reference resources are more often than not versioned (updated).\n\nStatic (OPEDAS) data sets from individual experiments may or may not be updated but can be changed over time. In any case, the influence of (re-analysis) of your data that changing OPEDAS sets or workflows may have should be considered in the study planning phase and appropriate measures should be taken to avoid undue surprises when re-using the 'same' OPEDAS resource at a later date. Questions to ask when analysing OPEDAS resources before reuse include; If the source is updated, will you do your analysis again?, What level of detail/granularity of the data do you need and will that be affected by updates? If you need only part of the data, Can you filter before downloading, and what is the subset you really need?\n\n## Do\n\n- Check the versioning policies of the OPEDAS source and consider the consequences\n- Decide what version to use\n- Decide what you do when updates are released\n- In case you always want to use a given version, make sure you will always have access to that\n- This means that if the source does not freeze versions, you may have to download, store and document the version you will use.\n- Make sure you extensively record and publish which version you used in which analysis\n- If possible, subscribe to updates of the resource so that you will be aware of updates.\n- If no update policy is clearly described at the source, try to find out the actual situation\n\n## Don't\n\n- Use versioned OPEDAS sources without recording the version and install a routine for updates\n- Assume that the OPEDAS owner/repository does have a proper versioning and documentation policy unless this is explicitly stated and described.\n- Publish results of your analysis without referring to the exact version of the OPEDAS resources you used for analysis.\n- Make any claims in your research output that are based on OPEDAS which may render the reference to the OPEDAS obsolete and/or confusing to the users of your research output.\n\n## Links\n\n- [DS Question GitHub resources repository: rgy](https://github.com/DSQResources/DSQ-rgy)\n"
    ]
  ,
    [ "ske"
    , "2.2.2"
    , "# Do you need to develop new terminology systems?\n\n## What's up?\n\nIf, after a thorough search you have to conclude that no suitable, or easily expandable, terminology system exists, you may (as a last resort only!) decide to embark on the development of a new terminology system. If that is unavoidable, you should at least follow the basic rules for the design of proper terminology systems. First of all, consider to contact stewards of existing and community adopted terminology systems to investigate whether they are open to extend their system with the new concepts you need to refer to in your data. This will greatly enhance exposure of the new identifiers and terms you add to the community and thus adoption of your extension.\n\nIf that all does not work, start with making a very simple, one dimensional list of concept terms you need to add to the new (addendum or) terminology system. Then, define the concepts as concisely and unambiguously as possible, add as many synonymous symbols of the term you can think of, also in other languages if that is in scope of the research, for instance extremely important in sample-banking research. Choose a proper persistent identifier approach to attach a persistent, machine resolvable identifier to each concept-denoting term. Finally, make sure the definition can be located (so you need a URI, which is not necessarily the same as a PID. If at all possible, do all this in a community of experts in the same domain.\n\n## Do\n\n- Make sure you have exhausted all possibilities to work with existing terminology systems or consortia before you ever start a new one.\n- Try to align with other domain experts that may have the same problem and try to make the broadest possible group (others are likely to make their own little term system again) There are fora to post these issues and a global community working on community development of standards and bst practices.\n- Keep it simple and open, just enough, just in time and widely announce you are doing this to try and avoid duplicate, time consuming and cumbersome projects.\n- If you need to make an ontology (add relationships) make sure your relationships (mostly 'predicate type' in linked data terms) are also well defined concepts, so that any 'triple' of the nature [subject] [predicate] [object] is a trits of three well defined concepts, each with a proper, persistent identifier.\n\n## Don't\n\n- Ever start an 'ontology' from scratch. First make a simple list of concepts you need to refer to and do not bother yet with their interrelationships.\n- Even make a thesaurus with hierarchical (narrower and broader terms) relationships unless you really need that.\n- Add relationships between the concepts in your list without defining these as precisely as the two concepts you want to link with that 'predicate' (see DO)\n- Assume that you can capture full reality in any practical ontology. Always other people will contest the types of relationships you chose because they have another perspective.\n\n## Links\n\n- [DS Question GitHub resources repository: ske](https://github.com/DSQResources/DSQ-ske)\n"
    ]
  ,
    [ "spg"
    , "2.3.1"
    , "# Did you consider how to monitor data Integrity?\n\n## What's up?\n\nBoth metadata and data themselves may be subject to change, intended or unintended. Electronic means of storing data and information does not guarantee its integrity. Apart from damage of the storage environment (to be addressed by redundancy and backups) the data itself may get unintentionally corrupted or changed. Also, data may be in formats that at some point in the future cannot any longer be easily 'read' by current software. In addition to those unintended changes, not all data and information is 'static' in nature. For instance, annotations of samples may have to change when the samples change. Having been taken out of frozen storage for instance may change characteristics and this has to be recorded. Also, user-defined metadata of data sets may change over time when usage of the data reveals new insights about the data. In addition, there are curated and summarising information resources that intrinsically change as they are updated when new data becomes available. Therefore, data integrity, versions of (meta-)data and updates need to be carefully monitored and regularly checked and monitored. *Nota Bene*: good stewards also record when their stocks are 'outdated', corrupted or otherwise no longer usable and 'intelligently dispose' of useless stocks, with proper reference and communication to users of the fact that a given data set is no longer available or 'archived'.\n\n## Do\n\n- Make sure the metadata of your data set is kept in different places (back up) and represented on catalogues where appropriate, but keep track of all versions the metadata.\n- Keep detailed logs and provenance of whatever happened to (meta-)data and/or samples after they were captured, preprocessed and archived initially. Especially when multiple copies of the data exist, make sure that derivative copies are not changed without recording the change in the metadata, so that people as well as machines will be made aware that they are now using a potentially changed version of the original data.\n- Record all actions that happened to archived physical samples, so that future users of (metadata) and the samples themselves will know exactly which version they are using.\n- Choose a FAIR-guided format for the metadata but also make strict versioning part of the infrastructure in which you store and provide the data.\n- Make (multiple) physically separated copies of the data, but keep very strict information of where data copies reside and how their integrity is guaranteed.\n- Keep master lists and metadata as closely associated with the original data or samples as possible.\n- Give specific people in the group access control rights and the responsibility to monitor data integrity at regular and frequent intervals.\n\n## Don't\n\n- Ever assume that data (let alone physical samples they may describe) will remain constant over longer periods of time without good care.\n- Leave data for long periods without regular attention, only to find out the they have been corrupted or used for the wrong purposes by others.\n- Let data be reused without a proper license also safeguarding data integrity rules. When workflows visit the data rather than entire copies of the set being downloaded for reuse elsewhere, workflows may make intentional or unintentional changes to the original data.\n- Throw away data (even if they are corrupted or obsolete) without keeping the original 'unique identifier' to the data in the international record. people and machines should know 'the data where there', what they were as well as when and how they were 'taken off line'.\n\n## Links\n\n- [DS Question GitHub resources repository: spg](https://github.com/DSQResources/DSQ-spg)\n"
    ]
  ,
    [ "brz"
    , "2.4.1"
    , "# Is all software for steps in your work flow properly maintained?\n\n## What's up?\n\nA frequent source of errors and lack of reproducibility is that variance in software used for pre-processing or the analysis of data. referring back to the term 'professorware' in the introduction, many software packages and algorithms are not even reaching that level and are custom made programs in coding languages that the informatician of the department happened to master.\n\nThe academic culture to press data scientists and engineers to publish about new algorithms and software packages drives the bias towards multiple, custom made workflows for identical research steps. However, to be the basis of reproducible results based on the same data, the workflow and all elements it uses (such as for instance thesauri for text mining, lists of reference data etc. need to be exactly the same. However, in many cases one or more of the components are not well documented, maintained or versioned (people you do not control decide on updates without even informing you). So, consider writing new software or algorithms for data analysis without the need to as a first capital sin for good data stewardship. Unless you are the very first to run a particular type of experiment, more likely than not software and workflows/pipelines for the type of processing and analysis you need will already exist.\n\nEven if you use 'existing' Open Source (OS) software produced in your scientific community, verify first that there is proper support, versioning and documentation about the code. If not, you run a serious risk to run into irreparable reproducibility problems very soon. In many cases, workflow decay in the public sector is a very serious problem. Next to that, many OS workflows are based on serial running of web service-type components. If one of those is 'off-line' or changed (without proper management and notice) your workflow will either not run properly or give unexplainable variations in results. It is therefore of the utmost importance and core business for data stewards to ensure that the software components you decide to use are of sufficient stability and quality to take the risk to subject your valuable data to them. Even for commercially provided and professionally engineered software and pipelines, there is always the risk that the company supporting the tool will have suboptimal versioning and support agreements in oplace and/or disappear from the scene. It is well established that tools that have already been evaluated as improper for the task they are used for continue to be used (and pass peer review) for years thereafter.\n\n## Do\n\n- Make sure you have studied the landscape of tools and services that meet your quality standards in terms of performance, stability, versioning, documentation and sustainability. In several fields there are tool registries and comments on the issues with particular tools or workflows.\n- Choose the most commonly used and validated tools available only and even in that case, document very carefully for yourself the date, the version and the conditions under which you have used the tools in your data processing activities.\n- Check also 'underlying components and plug ins' for their stability etc. As said before, if a text mining module changes the version of a terminology system between two runs, you will find very different results even if the text mining software itself is perfectly stable.\n- Contact the 'owner' of the software or tool whenever there is even the slightest doubt about all aspects mentioned above and make sure that future use of exactly the same composition and version can be guaranteed. When workflows or tools are only published recently and there is no evidence that they will be properly open sourced or otherwise sustainably provided in the future, think twice before using them.\n\n## Don't\n\n- Treat tools with any less care then the data themselves, which means that metadata about the tool, how you used it, which version, which components, when and with what exact input and output data should be captured and stewarded with the same care as described for data.\n- Develop any software or other data-tooling unless there is really no alternative. For instance, building a new tool that is 20% faster than a commercial alternative, just for the fun of it (or for a publication) and using that custom tool on valuable other than for tool evaluation *per se*, should be considered a no-go area for data stewards.\n- Think that OS tools (and your potential additions to them) are well supported just because they are OS. Unless there is a foundation or a company with some sustainability that supports the tool (OS or not) it is very likely that some time later, when the 'PhD student who made it moved on', the tool will either just go off line or give crappy or very different results."
    ]
  ,
    [ "rhm"
    , "2.3"
    , "# How will you be storing Metadata?\n\n## What's up?\n\nAlways consider the use of your data beyond the original purpose. One of the issues with searching for other people's data is that they can not be 'intrinsically indexed' as easily as for instance documents on the web. For that reason many data sets and databases go unnoticed and are at present heavily underutilised. This does not only hold for small and highly specialised databases. Even core resources such as the Human Protein Atlas are unknown to many researchers and much of the information in those databases is not readily available for indexing and all kinds of synonym problems preclude effective finding of such data and information sources. Many attempts are underway to develop data search engines, which are all challenged by the enormous variety in metadata standards, formats and data formats themselves. If all metadata were in FAIR format, the development of effective and highly performing search engines for all data sets available in the 'Internet of Data' would be infinitely more easy. Therefore it is important that you format your metadata according to the FAIR principles.\n\n## Do\n\n- Map all concepts you refer to in your metadata (including instruments, organisations and contributors etc. to community compliant terminology systems.\n- Choose a FAIR-guided format for the metadata.\n- Store and 'expose' the metadata in Open Access environments where search engines can easily find them, even of the data they describe are not (yet) FAIR or even highly restricted in access. The 'fact' that a data set with specific characteristics and contents is 'out there' is a first step toward effective reuse of your data or information source.\n\n## Don't\n\n- Make non-FAIR metadata and assume that search engines will anyway figure out where and what your data is.\n- Make FAIR metadata (in terms of format) but store them in a place where search engines have great difficulty to find them (off line, behind firewalls etc.).\n- Make metadata so minimal that even finding them gives the user (frequently a machine) no real clue what data or information is really 'there'.\n- Assume that with some 'hints' the rest will be obvious, rather assume the most 'ignorant' machine or person to try and figure out what your data set contains and is about.\n\n## Links\n\n- [DS Question GitHub resources repository: rhm](https://github.com/DSQResources/DSQ-rhm)\n"
    ]
  ,
    [ "tgk"
    , "2.5.1.3"
    , "# If your data changes over time, how frequently do you do backups?\n\n## What's up?\n\nData may change over time, Not only curated databases that get updated 'as a routine' but errors may be spotted and corrected (not necessarily only wrong values but maybe misplacing in the wrong column etc.). It is extremely important to record any post-capture and initial archiving changes and to also reflect these in an immediate backup of the changed data. The provenance should also enable to 'go back' to the original data, even if these were deemed inappropriate in hindsight. Reviewers of the conclusions should for instance be able to trace why initial conclusions were revised.\n\n## Do\n\n- Backup old and new versions of data.\n- Keep exact and rich provenance of all changes post-initial generation.\n- Explain why data have been adapted, curated, cleaned etc. and make sure also workflows 'know' which version of the data to use.\n- give each new version of a data set a new PID.\n\n## Don't\n\n- Ever change anything in recorded data without informing the original data owner.\n- Make changes (even obvious and minimal corrections) without recording and documenting that change.\n- Make a change without preserving both the old and the new versions of the data with the appropriate metadata and back ups.\n\n## Links\n\n- [DS Question GitHub resources repository: tgk](https://github.com/DSQResources/DSQ-tgk)\n"
    ]
  ,
    [ "wht"
    , "1.10"
    , "# Do you need to harmonize different sources of OPEDAS?\n\n## What's up?\n\nOnce you have decided which OPEDAS resources you wish to use for your study, there is several questions to be asked regarding different data formats, harmonisation issues and for instance the use of identifiers and terminology used in the data. If these questions are not addressed up from they may severely delay or even completely jeopardise the study.\n\n## Do\n\n- Check the format in which the actual data elements are presented refer back to 1.5 and 1.6 for related issues\n- Check the terminology systems [definition needed to avoid 'ontology' abuse-language] used for each data set and discuss consequences for analysis pipelines etc.\n- Decide how the formats of the OPEDAS resources and the ontologies used will influence your choices regarding your own data capture\n- Check: If metadata or data elements contain natural language fields, do they need translation or\n- Mapping to ontologies\n- Where possible, consider to reformat data to make them 'linkable' and report back to OPEDAS owner\n \n## Don't\n\n- Expect data (even if in F.A.I.R. format) to be reusable in your setting without any reformatting.\n- compare 'apples and oranges' because data are not properly harmonised and might give the wrong correlations\n- change anything in OPEDAS sets (formats, translation of terms, mapping to term systems) without proper documentation, provenance and report to the OPEDAS owner.\n\n## Links\n\n- [DS Question GitHub resources repository: wht](https://github.com/DSQResources/DSQ-wht)\n"
    ]
  ,
    [ "wya"
    , "1.7"
    , "# Will you be using any existing (non-reference) data sets?\n\n## What's up?\n\nIn case some of the OPEDAS sources you want to use are not qualified as 'reference data sets' and/or are not available in TDR type repositories where access, ownership, sustainability and versioning is well documented, a number of extra questions need to be answered before you can responsibly reuse the data for your analysis\n\n## Do\n\n- Check the conditions under which you can get access\n- Check ownership\n- Check potential restrictions (for example commercial use, which might come in much later)\n- Decide what version to use (if versioned) and consider all options discussed in 1.6.\n- Double check with the owners for how long they intend or guarantee to keep the data available in the same format and version.\n\n## Don't\n\n- Expect that data that are not explicitly deposited in a trustworthy, sustainable environment will be there when you next visit.\n- Use OPEDAS indiscriminately, without detailed recording or their origin.\n- Expect your data analysis workflows to give reproducible results unless you guarantee stable input, including the OPEDAS-dependent elements of your analysis pipeline.\n\n## Links\n\n- [DS Question GitHub resources repository: wya](https://github.com/DSQResources/DSQ-wya)\n"
    ]
  ,
    [ "xyf"
    , "4.2.3"
    , "# Can workflow decay be managed?\n\n## What's up?\n\nThe stability of workflows over time is a matter of considerable concern and debate (see also generic concerns in the introduction section). In a recent publication (2015) it was shown that workflows collected in a well-established workflow support environment (Taverna) are for a serious percentage not re-executable, and often the cause is in rather trivial shortcomings, such as lack of example values needed as workflow input parameters, as well as missing libraries for Java programs. The practical consequence of the use of 'academic' workflows, especially those that do not have a frequent use, frequently have no Service Level Agreement attached, while the code is probably also sub-optimal in terms of performance, scalability and supportability aspects.\n\n## Do\n\n- Check the decay risk for each workflow you intend to use before making it part of your routine data processing pipeline.\n- Contact the original developers and ask them the pertinent questions about documentation, versioning, support and sustainability.\n- If any of these are unsatisfactory, discuss with the team whether the risks for reproducibility, review and quality of the downstream data (output from the workflow) are acceptable.\n\n## Don't\n\n- Assume that workflows that are offered on the web even if they are formally published in informatics journals and part of a workflow environment or presented in for instance a Galaxy setting are still the same as when they were first published.\n- Believe the sustainability and support claims for either academic or commercial workflow systems without due diligence (including interviewing prior expert users).\n- Contribute to the unstable workflow jungle by adding your own professorware-solution to the mix without proper consideration of support and decay issues.\n\n## Links\n\n- [DS Question GitHub resources repository: xyf](https://github.com/DSQResources/DSQ-xyf)\n"
    ]
  ,
    [ "kdp"
    , "2.5.5"
    , "# How long does the data need to be kept?\n\n## What's up?\n\nA good steward will not keep the goods to be taken care of beyond their 'expiration date'.\n\nFor data stewards this means that data (or subsets, such as intermediate formats, original images etc.) do not necessarily all have to be stored and/or published. This is not a trivial issue. For some large data sets it is far from easy or cheap to 'just keep everything'. So the first consideration here is to determine which phases of the data generated and processed need to be 'kept-forever-in-principle'. This means that even in the process of generation and processing of the data there might be already files that can responsibly be disposed of. next, some data can be 'zipped' in the sense that they can be stored in a reduced format that allows to reconstruct the richer format without information loss or the introduction of errors. An example of such a steward-decision is the question to either store the entire sets of reads and the full imputed genome sequence of an organism as opposed to just the 'variations from the reference genome'.\n\nIn addition to these early process decisions, there may be a point when archived data appear to be not re-usable anymore. It is important to distinguish here between 'not used anymore' and 'principally unusable'. If data appear to be corrupted, outdated, falsified or in other ways 'misleading or useless for further research' there may be a point where the data steward decided to dispose of the data.\n\nEven in that case the metadata and the persistent identifier should be kept, as the community should be able to trace the original data and be aware of/refer to the fact that the data are no longer available.\n\n## Do\n\n- Discuss a long-term data sustainability and reuse plan with the team for each data set generated, downloaded or acquired.\n- Plan for a budget and regular 'expiration options' meetings for all data sets under your stewardship.\n- Transfer these plans, resources and responsibilities explicitly to the trust third party you may give stewardship over your data.\n- Make sure that even if data are 'destroyed' or 'tape-archived', the metadata as well as the unique (citable) identifier always stay FAIR.\n\n## Don't\n\n- Mix reusability with actual reuse: there are many examples where intrinsically valuable data had not been used for many years and suddenly appeared to be crucial for a particular study or decision. Similarly there are examples of data that have been lost and would now be very valuable.\n- Ever throw away data, metadata, code or tools without informing the team and discussing the decisions. It is not always easy for the data steward to determine the actual expiration date of data.\n- Keep data (even small sets) when there is clear evidence that they are corrupted, false, wrong or obsolete for other reasons, even if there is no 'storage' or financial reason to delete them.\n\n## Links\n\n- [DS Question GitHub resources repository: kdp](https://github.com/DSQResources/DSQ-kdp)\n"
    ]
  ,
    [ "wia"
    , "2.7"
    , "# Do you need the storage close to compute capacity?\n\n## What's up?\n\nThere is a transitional debate ongoing about distributed trust and learning networks. This is not only pertaining to to scientific realms, but is critically addressed in next generation solutions to bitcoin, blockchain and other approaches. The essence of the Internet is local transactions that are internally and intrinsically robust (including accepting error prone issues and redundancy only where redundancy makes sense). This trend will rapidly influence 'local' versus 'distributed' compute and learning. So in any case, for a small or a large study, you have to consider the possibilities of local versus distributed computing. The options range literally from using smart-phones to distributed supercomputers heating homes and connected via glass fiber to exascale computer facilities. It is not a 'given' anymore that massive compute and analysis jobs have to be done centrally and within firewalls even when sensitive data are involved. If we define the 'cloud' as 'other people's computers' in general, most of what you need to do may be perfectly feasible 'in the cloud'. However, there may be reasons to do things locally and maintain expensive compute infrastructure. Still, never buy locally maintained hardware if there are better and cheaper options 'just for the sake of controlling it'. No-one would generate electricity or clean water anymore unless as a backup generator or in case the water needs to be of exceptional quality that can not be delivered from external sources. Compute and Storage rapidly becomes such a 'general commodity' and using it externally or in the 'cloud' becomes a major issue to address. The core question is: how much 'compute' capacity needs to be immediately associated with the data. Recognising the speed of current connections in your network is obviously a major aspect here. This again pertains to whether you plan to send massive data sets over networks (where network speed may still be a major cost and time issue) or 'light weight' workflow containers that visit data *in situ* to do relatively restricted compute job.\n\nThere are emerging trends that show that many more scientific questions can be answered by distributed analytics and learning than we would intuitively believe. Therefore it is considered bad data stewardship if you burden our institution with internal hardware and software issues that can be done by trusted third parties much better, cheaper and safer.\n\n## Do\n\n- Study and consult carefully on cloud based options for the storage and compute you will need.\n- Compare different cloud services providers for pricing, service and security (bigger is not always better).\n- Make sure you are allowed to 'send data around' before going down that path.\n- Consider that up- and download pricing of regular cloud providers will be very considerable of you deal with really large data sets.\n- Balance the costs you will be incurring using a cloud provider with the costs of local facilities\n- Calculate the risks of procurement, maintenance, support, expertise, and renewal of machinery and infrastructure in your own institution as well.\n- Choose the right local/distributed approach for each individual workspace and 'batch' of analysis (not a one size-fits-all approach).\n\n## Don't\n\n- Just assume that the storage and data processing will be done 'in house' without a very strong justification for that (major costs and logistics involved).\n- Buy or hire much more computer space and power than you actually need.\n- Consider every compute job a 'compute and data are altogether' situation.\n- Confuse 'large scale compute problems' with 'large scale' local infrastructure.\n\n## Links\n\n- [DS Question GitHub resources repository: wia](https://github.com/DSQResources/DSQ-wia)\n"
    ]
  ,
    [ "ybd"
    , "2.5.3"
    , "# Re-use considerations: Will the archive need to be online?\n\n## What's up?\n\nThere is a very serious difference between 'offline' archives (obviously with online FAIR metadata ) and online (and ready-for-reuse) data archives. Actually, it is considered better to reserve the term 'archive' for the situation where the actual data are 'offline' and if found based on the FAIR metadata can be retrieved from the archive and made 're-usable on demand' and to distinguish those 'archives' from 'high performance re-usability' (HPR) environments where data is kept in a 'poised for frequent reuse' format. The latter puts many more requirements on the bandwidth, up-time and support of the infrastructure in which the data are offered.\n\n## Do\n\n- Always make the distinction between 'offline archiving' for reuse upon request and a 'high performance reusability' (HPR) environment.\n- Realise that the latter may be an order of magnitude more costly than the former.\n- Budget adequately for either form of 'archiving' after the conclusion of the experimental procedures and analytics with the data was originally generated and interpreted.\n- Consider the potential need of users to have 24/7 access to the data, which is usually beyond the capacity and mandate of academic institutions.\n\n## Don't\n\n- Put data in long term HPR environments unless you have strong expectations or evidence that they will be reused intensively.\n- Assume that you can offer data for reuse by just 'putting them somewhere of people to download. The offer of re-usable data comes with stewardship responsibilities and sometimes with many questions for which you have to be prepared.\n- Consider your group a 'local HPR' node without clear mandates and personnel. You may want to budget for 'handing over' your data to a trusted and professional repository or HPR environment.\n\n## Links\n\n- [DS Question GitHub resources repository: ybd](https://github.com/DSQResources/DSQ-ybd)\n"
    ]
  ,
    [ "zmu"
    , "2.5.6"
    , "# Will the data be understandable after a long time?\n\n## What's up?\n\nDigital objects are not protected from decay or from lack of actionability. Ever tried to read a floppy disk recently? The evolution of data storage, retrieval and processing are developing so fast that, contrary to what a lay person may expect, digital objects become obsolete (unreadable) much faster than classical written text on paper or on microfiche. Communication in human language has obviously evolved as well and reading text of many ages old is not without its difficulties. Not only because of changes in spelling and style, but also because of semantic drift.\n\nHowever, recovering files from only decades old that have not been updated to current formats is already a challenge in many cases. Therefore, especially for those data sets that are too large to effectively store and reuse in 'PDF' type settings a long term plan with regular 'checks' for readability and reusability is needed.\n\n## Do\n\n- Discuss a long-term data sustainability and reuse plan with the team for each data set generated, downloaded or acquired.\n- Plan for a budget and regular 'expiration options' meetings for all data sets under your stewardship.\n- Transfer these plans, resources and responsibilities explicitly to the trusted third party you may give stewardship over your data.\n- Make sure that even if data are 'destroyed' or 'tape-archived', the metadata as well as the unique identifier always stay FAIR.\n\n## Don't\n\n- Mix re-usability with actual reuse: there are many examples where intrinsically valuable data had not been used for many years and suddenly appeared to be critical. Similarly there are examples of data that have been lost and would now be very valuable.\n- Ever throw away data, metadata, code or tools without informing the team and discussing the decisions. It is not always easy for the data steward to determine the actual expiration date of data.\n- keep data (even small sets) when there is clear evidence that they are corrupted, false, wrong or obsolete for other reasons, even if there is no 'storage' or financial reason to delete them.\n\n## Links\n\n- [DS Question GitHub resources repository: zmu](https://github.com/DSQResources/DSQ-zmu)\n"
    ]
  ,
    [ "dta"
    , "2.5.4"
    , "# Will workflows need to be run locally on the stored data?\n\n## What's up?\n\nMany data sets will either be too large to be effectively and economically be 'shipped around' even if they are offered for reuse. In addition, there may be privacy, legal or commercial restrictions that prevent the data from physically leaving your internal repository'. These issues are extremely important to be carefully studied at the early stage of data generation, as they may influence your generation, licensing and stewardship choices. If data cannot be 'shipped' the environment in which you offer them for reuse is critically important. A 'download server' where people can just 'take the data for their purpose' is very different from a place where 'workflows' (compute elements) can come in and do their calculations and analytics on your data set locally. Security and authentication issues (here pertaining to visiting workflows) are very different and also there should be sufficient compute power directly associated with the data for the workflows to do their work efficiently. The licensing to the data and what may or may not be done with the results of the local computation have to be very clearly defined and a support mechanism for the '*in situ*' reuse of the data should be in place. Again this may be outsourced to a trusted party. Also consider that the metadata of your data set may be perfectly provided to external parties as opposed to the data itself.\n\n## Do\n\n- Discuss the licensing and security issues related to data to be produced before capture wherever possible, as these constraints may influence your meta data capture strategies, your formatting and the budget of your experiment very significantly.\n- Consult with experienced people in the area of workflows and distributed learning to understand what the requirements would be to make your data optimally re-usable for 'visiting software' or workflows in general.\n- Test run workflows on your data to confirm that they are indeed 'accessible' for workflows.\n- Include rich metadata on the format, the constraints and the usability of the data, for instance which identifier scheme was used internally and warnings that the data are not 'ready for use' for particular workflows.\n\n## Don't\n\n- Capture data without having considered the perceived needs for reuse and in particular whether that is classical data sharing or access to your data '*in situ*'.\n- Assume that 'someone else' in your institute will take care of the infrastructure and support the reuse of your data by third parties, but ensure that this is well organised or budget for internal or external services.\n- Waste time of the support people (internal or external) by not being prepared to answer questions about reuse conditions (download versus *in situ*, up-time guarantees needed, security level, licenses etc.)\n"
    ]
  ,
    [ "kqh"
    , "2.5.4.1"
    , "# Is there budget to enable supported reuse by others (collaboration/co-authorship)?\n\n## What's up?\n\nIt is very important when planning for the adequate budgeting of your data stewardship plan to think beyond the use of the data for your own study. Here we re-introduce the term 'data publishing'. As we should treat re-usable data sets with the same care as we are used to for research articles, we need to make sure that optimal use by others, citation and sustainability are all covered and properly budgeted for. For research articles, it has long been accepted that external parties take part (for a fee in Open Access) in the formatting, redaction, peer review, publishing and preservation of your narrative article and the accompanying resources (supplements). Think about data in the same way. They are a valuable output of your research and should be 'published' in their own right, regardless of any articles (sometimes more than one) you might want to base on the data. Obviously, the description of the data should be rich enough to make them actually re-usable by others. That means aspects of Findability (including good metadata and a persistent identifier), Accessibility (Open Access, restricted, license as well as 'download-ability' or '*in situ* accessibility' for analytics. In the 'article' sphere, interoperability of the outcome was largely related to 'readability for other human users' and this again is largely confined to proper language, rhetoric and formatting, issues that are usually co-judged and improved in the 'peer-review' process. However, interoperability of data is more complicated. Text is a nightmare for computers as we already described, so machine readable and actionable met data formats and data formats are critical. For most data types standard formats, and increasingly also metadata format are available and these should be used wherever possible. Human reviewers will increasingly be unable to 'check every line' of your big data sets and therefore will have to rely on the formats and standards you used. We expect more and more computable 'quality check and integrity check' tools in the data space but these are not yet available for all data types.\n\nThis all means that your study plan (or proposal) should adequately budget for data publication in FAIR format. Tools are under development to certify your (meta-)data as FAIR but these are in their infancy and therefore you may need to spend considerable time, effort and funds to get your data in the correct format, to have it checked and to actually publish it. The good news is that these publishing costs are mostly 'eligible costs' in research proposals and they also include the long-term archiving of these data. One data are in the correct format and 'static' in size and shape, the long-term storage can usually be easily budgeted and justified.\n\n## Do\n\n- Carefully budget for the publication of your data.\n- Check various data publishing providers and check whether they are trustworthy (for instance approved/certified by the funder of your research).\n- Compare prices of different publishers and specifically also whether they include long-term preservation of the published data.\n- Restrict this part of your budgeting effort to the actual 'archiving' as a basic preservation cost.\n- Budget separately for potential storage and copies of the data in HPR environment, which might be very worthwhile to increase reuse and citations on your data and thus 'impact'.\n- Check whether your institution has a data access committee or an equivalent body.\n\n## Don't\n\n- Mix one-off formatting, review, quality checks etc. with real 'publishing' of the data in a format and environment that will make the FAIR (with again the main consideration being machines, the prime 'users' of digital large data sets.\n- Assume that an Open Source, Open Access Academic repository is always the best option for the funder and the actual reuse of your data. Many of these are not Findable or Sustainable, both preconditions for FAIR.\n- Mix publishing fees and budget with budgets for reuse (by others).Third parties who want to reuse your data would have to budget for reuse costs (such as download fees, re-formatting etc.)in their grants. It is not your responsibility to take part in these costs, it is your responsibility to offer the data for reuse under well defined conditions (which might in some cases be very restrictive or even commercial).\n"
    ]
  ,
    [ "ijn"
    , "2.8.1"
    , "# Determine needs in Memory/CPU/IO ratios\n\n## What's up?\n\nThe ratio of storage versus compute related capacity will vary per project and data/analysis combination. However, at a very general level, the balance will only vary within certain boundaries (exceptions will always be found). Regardless of whether massive data sets are locally collected, stored and processed by massive amounts of CPU's or whether thousands of distributed data sets will be visited by workflows to do relatively simple compute jobs on these data, there will always be a need to determine the size of the data, the complexity of the individual compute job and consequently this balance between storage and compute. If you do not properly study, pilot, test run and decide on these issues based on current expertise and evidence, you may 'buy' stuff you do not need and put a heavy maintenance burden in your institution/colleagues. Conversely, if you underestimate the complexity of these matters, your study may face severe hurdles and delays or even data looser corruption down the road. So take the appropriate time and measures to make sure the decisions made are the best based on current knowledge and funding.\n\n## Do\n\n- Study and consult carefully on cloud based options for the storage and compute you will need.\n- Compare different cloud services providers for pricing, service and security (bigger is not always better).\n- Make sure you are allowed to 'send data around' before going down that path.\n- Consider that up- and download pricing of regular cloud providers will be very considerable of you deal with very large data sets.\n- Balance the costs you will be incurring using a cloud provider with the costs of local facilities\n- calculate the risks of procurement, maintenance, support, expertise, and renewal of machinery and infrastructure in your own institution as well.\n- Choose the right local/distributed approach for each individual workspace and 'batch' of analysis (not a one size-fits-all approach).\n\n## Don't\n\n- Ever just assume that the storage and data processing will be done 'in house' without a very strong justification for that (major costs and logistics involved)\n- Buy or hire much more computer space and power than you actually need.\n- Consider every compute job a 'compute and data are altogether' situation\n- Confuse 'large scale compute problems' with 'large scale' local infrastructure.\n"
    ]
  ,
    [ "rqh"
    , "3.2"
    , "# Capacity and harmonisation planning\n\n## What's up?\n\nOnce you know exactly what the output of the experiments or studies will be, both in terms of data types and their volumes, you need to plan for the required capacity to capture, store, process, analyse and provide them for reuse. These aspects have been covered in general terms before but here we need to emphasize that it is your task as a data steward to prevent unexpected capacity problems in hardware, but also human capacity that may cause delays or even waste of generated data once the experiments have been set in motion. This does not only cover the capturing of the raw data, although this is the most urgent issue to address. You should also be fully aware of the stability of data and samples generated in each step of the experimental process. In many cases, samples taken from physical or biological systems for example have a very limited stability and measurements on them can be heavily influenced by the time lapse between the time point at which the sample was taken and the time the actual measurement of its features and values took place. Again, this is mainly a part of general good research practice and the co-responsibility of the entire team, but your role as a data steward is to maximally ensure that variations that may be traceable to time-lapse or other experimental conditions are properly recorded and therefore traceable. For instance the exact position of each sample in the container used for measurements (for instance a 96-well plate) and the time it took to run the entire plate through the measurement procedure can prove to be extremely important for later processing and analysis of the data. If it is well established that for instance fluorescence of a marker has a given decay over time, differences between the first well in the plate and the last one may have to be corrected for that before the data can be processed further to be meaningful.\n\n## Do\n\n- Make sure you fully understand the 'sources of potential variance' that are relevant for the experimental study at hand and that you advise the research team on which metadata and process descriptive data have to be captured to minimise risk of data loss or lack of harmonisation, calibration and correction procedures.\n- Capture all reasonably possible metadata and data about laboratory procedures again following the 'better- throw-away-than-regret-not-to-have' principle.\n- Ensure that correction, harmonisation and calibration procedures on the data are also considered as part of the experimental process and therefore monitored and captured as such.\n\n## Don't\n\n- Consider yourself out of loop during the actual experiment and see yourself merely as the recipient of the data when they come out of the experimental workflow process.\n- Consider the deep knowledge about experimental procedures (not necessarily the hypothesis being tested or generated) none of your business and the realm of the experimentalist, but act as an intrinsic member of the research team, also during this step of the data cycle.\n- Start the actual data capture process without the best possible knowledge about the future use of the data and the requirements to be able to compare them with OPEDAS without cumbersome and error prone *post hoc* harmonization processes.\n"
    ]
  ,
    [ "nkj"
    , "3.1"
    , "# Where does the data come from? Who will need the data?\n\n## What's up?\n\nThe use of the data after the capture process may seriously influence the decisions made before and during data capture. In all cases, data quality should be as optimal as possible given the experimental conditions or the boundaries of the study. Still, data may be generated in some cases for very limited purposes (for instance the calibration of an experiment) or for massive reuse (sequencing a reference population on biodiversity or climate change data points) and as said sometimes for highly regulated and certification purposes. The richness of the metadata needed to enable these post-capture processes and needs must be carefully considered before and monitored during the actual data capture process.\n\n## Do\n\n- Discuss with the PI and the team the exact sources of the data as well as their purpose.\n- Point out to the team that they also have to anticipate use beyond the original purpose of the data.\n- capture the richest possible metadata within your experimental and institutional possibilities (better discard some metadata later than wishing you had captured them).\n- Clearly divide the experimental results in data that can principally be made FAIR. (i.e. also machine actionable) and those that are intrinsically not machine readable. Examples of the latter category are for instance environmental samples, tissue samples, biological specimens, sound recordings, videos, free text etc.\n- Make a very serious effort to adorn the non-FAIR. experimental results (regardless of whether they may be made intrinsically FAIR later on) with rich FAIR metadata, annotations and provenance information.\n- Make a full list of reagents, sensors, workflows, machines and other 'research objects' that is involved in the process of data generation.\n- Agree with the team about the level of metadata needed for each step in the study process.\n- Record the batch and version numbers of all research objects and reagents used in the process.\n- Carefully study whether the data capture, storage and exchange formats have been settled before in previous experiments, either in your team or elsewhere.\n- Make a backup of all metadata in digital format, even if they are also written on a tube) with a PID for every sample and a link between that PID and the actual sample (if the PID can be sustainably engraved in the sample container, that is obviously the best choice).\n\n## Don't\n\n- Consider experimental results or objects that cannot be made intrinsically Findable and Actionable for machines as 'unfair' in the semantic sense. They are key scientific research objects but they are of a nature that makes them not usable for machines (and in many cases humans) without intermediate steps (a tissue sample may reside in a freezer in a plastic tube). However, the metadata describing it will be crucial to make the (set of) samples Findable, Accessible (with intermediate steps) Interoperable (after processing) and therefore ultimate Re-usable by others.\n- Assume that the purpose for which samples or data where generated originally will be the only purpose for which they will ever be used. Instead, try to imagine any other future use to the best of your abilities.\n- Separate the metadata any further from the non-FAIR data and samples any further than absolutely necessary to support minimal chance of mix-ups (labels falling off tubes in the freezer-type).\n"
    ]
  ,
    [ "wqm"
    , "6.5.1"
    , "# Will this step need significant storage and compute capacity?\n\n## What's up?\n\nThe processing and storage of data is no longer a trivial issue as far as compute and storage capacity is concerned. Later we will explain the vast difference between data 'archiving', 'publishing' and offering data in 'high performance analytics environments'. For now, it is important to realise that turning vast datasets into their final processed form, publishing them and offering them for effective reuse may put significant burden on your ICT and expertise. So this is an aspect to consider already in step 1 of the data stewardship cycle.\n\n## Do\n\n- Consider data size and complexity of the data to be generated in the experimental design process\n- Include the perceived needs for data archiving, but also internal and external reuse issues in the capacity planning\n- Clearly distinguish between 'first-off' use of the data for the study and the needs of peer reviewers to reuse the data for reproducibility checks only and the wider community that may want to use your data as OPEDAS for entirely different purposes.\n- Plan and budget for both as just providing your data for reproducibility checks will usually be not enough for a good data stewardship plan.\n- (see resources, cloud providers, FDG etc.)\n\n## Don't\n\n- Underestimate the time and resources you need to make the data FAIR for reproducibility and other reuse purposes.\n- run processes leading to subsequent stages and formats of preprocessed data on unspecified and 'local machines' and with undefined software (see general provenance rules for data processing).\n"
    ]
  ,
    [ "fxm"
    , "5.2"
    , "# Will you make your output semantically interoperable data?\n\n## What's up?\n\nIn case you decide that the data type you want to link or integrate is 'semantic' in character, the linked data approach (regardless of the schema and format chosen) should be maximally guided by the FAIR principles. One frequent misconception is that FAIR is the same as Linked Data and Linked Data is the same as 'RDF' or 'Semantic Web. The FAIR principles do not demand any of that. they simply ask from you that you take optimal care of the four elements of the FAIR principles (for full context see http://www.nature.com/articles/sdata201618). Please recognise once more before studying the principles below that they may only pertain to the metadata or the annotations of otherwise none machine recognisable or actionable objects. In that case a unique and persistent identifier should be assigned to the physical object (a tube, a piece of text, a picture, a person) and the metadata should be persistently linked to the physical object, so that it becomes Findable and Accessible for Re-use, even if there may be several steps in between before the actual data become interoperable, linkable or integrated for the study you want to use them for. This may even include re-analysis, further measurements (looking for new metabolites in an old sample that were not measured before) but the fact that the object that can be a relevant source of data is available under well defined conditions for reuse in research should be adequately described in the metadata and annotations associated with the object.\n\nTo be Findable:\n\nF1. (meta)data are assigned a globally unique and persistent identifier\n\nF2. data are described with rich metadata (defined by R1 below)\n\nF3. metadata clearly and explicitly include the identifier of the data it describes\n\nF4. (meta)data are registered or indexed in a searchable resource\n\nTo be Accessible:\n\nA1. (meta)data are retrievable by their identifier using a standardised communications protocol\n\nA1.1 the protocol is open, free, and universally implementable\n\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\n\nA2. metadata are accessible, even when the data are no longer available\n\nTo be Interoperable:\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\n\nI2. (meta)data use vocabularies that follow FAIR principles\n\nI3. (meta)data include qualified references to other (meta)data\n\nTo be Reusable:\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes\n\nR1.1. (meta)data are released with a clear and accessible data usage license\n\nR1.2. (meta)data are associated with detailed provenance\n\nR1.3. (meta)data meet domain-relevant community standards\n\n## Do\n\n- Make a careful analysis of the objects and data that can be made FAIR and those that can only be made part of the FAIR ecosystem through semantic annotations and metadata.\n- Create the metadata and identifiers according to the FAIR principles, regardless of whether the data or the object/data source itself can (also) be made FAIR or not.\n- Follow the FAIR principles for all data types that can be made machine actionable.\n- Consider the guiding principles in every step, for instance (R1.3) by choosing a vocabulary that meets (or is properly mapped to) domain-relevant community standards.\n- Always keep in mind that the end goal of FAIR data creation and publishing is optimal Re-use for your own and others' studies that require functional interlinking or integration of multiple data sets.\n- Keep and publish annotations on existing data sets that you reused (link to CEDAR) and add them to the FAIR ecosystem by exposing them in a FAIRpoint or a FAIRport (see chapter 7) even if you are not the original creator of that data.\n- Study and implement where possible the 'Joint Data Citation principles' in order to make your data maximally reusable and citable, so that your institution can do metrics on and reward you for offering your data and annotation for reuse by others.\n\n## Don't\n\n- Try to make data or other research objects FAIR that are intrinsically not machine actionable\n- Make data machine actionable without considering a proper (and preferably unambiguous' human readable symbol for each concept (URI's are as annoying to people as human language is to computers)\n- Allow any 'concept' denoting symbol in your (meta)data that is 'ambiguous' and will therefore frustrate both *in silico* and *in cerebro* reuse by others.\n\n## Links\n\n- [DS Question GitHub resources repository: fxm](https://github.com/DSQResources/DSQ-fxm)\n"
    ]
  ,
    [ "pzq"
    , "4.4"
    , "# Tools and data Directory (for the experiment)\n\n## What's up?\n\nA full directory of all tools, workflows and data collections used in your experiments should be available to all members of the research teams at all times.\n\nNot only should you avoid to become a single point of failure in the data processing phase of the research and data cycle, it is also very important and time saving that you maintain a professional and easily accessible directory of all data processing tools used in each experiment. The best way to do this is to have a general directory where optimal metadata and provenance about the tools and their components is provided. Each tool (component) should be given a unique identifier (including different versions) and for each individual experiment you can refer to this general directory for the tools that have been actually used (and at which point and how) in the experiment at hand. This is especially important for distributed projects. Make sure you run a subset of the workflow/data combinations on all infrastructures to ensure consistency. Making pipelines portable across work spaces (ultimately as VM-type containers) reduces the risk that results will be different in different sub work spaces of the distributed team.\n\n## Do\n\n- Keep a general directory of all tools used in all experiments and make sure all team members know of it.\n- Make sure that all team members can access the directory and refer to it properly in their lab notebooks and at any other time.\n- Ensure that your directory is stable and professional enough to serve as a formal repository of tools that can be used in reviews and reproducibility checks.\n- Make a Contingency Plan for workflow errors, including alternatives (e.g. cluster, grid, cloud)\n \n## Don't\n\n- Run any workflow on any data without referring to it's generic representation in the directory.\n- Run workflows without recording the exact version used (this can be best approached by having a PID for each version of each tool in the general directory).\n- Describe third party tools in the general directory with your own words without adding sufficient metadata, provenance and links for all members of the team to drill down to the origins of the tool.\n\n## Links\n\n- [DS Question GitHub resources repository: pzqa (Will you use a central repository for all tools and their versions as used in your project?)](https://github.com/DSQResources/DSQ-pzqa)\n- [DS Question GitHub resources repository: pzqb (Will you use a central repository for reference data used in your project?)](https://github.com/DSQResources/DSQ-pzqb)\n"
    ]
  ,
    [ "qqb"
    , "5.3"
    , "# Will you use a workflow e.g. with tools for database access or conversion?\n\n## What's up?\n\nworkflows for FAIR data handling are increasingly available. As discussed in section 4, for most data types and metadata needs there will be earlier examples, community emerging standards and publication as well as linking, integration and analytics workflows that can handle FAIR data.\n\nAs of 2014, a growing series of so called 'Bring Your Own Data workshops have been organised where data owners of various basic data types have worked with FAIR data experts to represent their original data in a FAIR manner at the metadata level and in many cases also to turn (part of) of their core data into FAIR format. In most cases this procedure did not*replace* their original database, but it merely exposed part of their data in a FAIRpoint, meaning that FAIR-aware tools will find them and will be able to operate in that subset of the data. By the time you read this book, many data types will have been 'done before' and an increasing number of concepts you will have to refer to in your (meta)data will be covered by community compliant and supported vocabularies. There are international efforts and research infrastructures in most domains that are in full swing to develop tools, standards and best practices for their domain where these do not exist. Granted there may be large differences in maturity of data type registries, proper, controlled vocabularies, mapping services between those and ontologies dependent on the scientific domain, but increasingly, front line research will have to link and meta-analyse data from different domains and disciplines. It is therefore imperative for interdisciplinary and open science that your data 'talk optimally' to other data within your domain and also to data from other domains. Therefore, your search for 'pre-existing best practices, formats and standards' should not stop at your disciplinary border. One of the best approaches to get acquainted with best practices of the data type you have at hand is to turn to establish research infrastructures in your domain or consult certified data stewardship nodes in your domain, or for instance the Research Data Alliance.\n\n## Do\n\n- Consult experts, preferably associated with established research infrastructures in your domain.\n- Consult also outside your domain (there may be a vocabulary in agriscience you may want to extend for a biodiversity project, or one in meteorology you may be able to use for part of your geology data, as well as one in chemistry that may cover the metabolites you want to refer to).\n- Detect concepts or formats that you need and for which no appropriate 'prior art' is Findable or Re-usable in your specific case.\n- Always approach research infrastructures or services (like ELIXIR, NCBO or EPOS) for advice. In many cases customisation of existing data models and formats or extension of leading ontologies will be the best option to cover these identified gaps.\n\n## Don't\n\n- Invent new data types, linking, publication or integration pipelines unless you have exhausted all possibilities to use or customise existing ones.\n- Create new identifiers for any concept in your data set unless the concept is nowhere defined in the exact defined meaning you want to attach to it. If you really need to create a new identifier for a new concept (for instance a phenomenon or a species you describe for the first time in the scientific discourse) always try the route of extending community preferred vocabularies and ontologies first.\n- Link and integrate data sets that are intrinsically non-harmonised and incomparable or incompatible. Some inconsistencies will automatically be detected by computer programmes but the 'crap in crap out' mantra is true also for the meta-analysis of linked or integrated data.\n\n## Links\n\n- [DS Question GitHub resources repository: qqb](https://github.com/DSQResources/DSQ-qqb)\n"
    ]
  ,
    [ "xke"
    , "7.4"
    , "# What technical issues are associated with HPR?\n\n## What's up?\n\nOffering your data in a 'HPR' environment is much more costly than just publishing them. However, there might be several (and rapidly accumulating) reasons to choose for a HPR environment anyway. First, in case you are internally rewarded for the reuse of your data (unfortunately only a fledgling practice to date) there may be great benefits in choosing a HPR, as the chances that others will actually reuse and cite your data will be naturally (much) higher. Second, the participation in a FAIR HPR environment may give you extremely valuable feedback on your data over time. Not only through reuse *per se*, but for instance also through annotation (by others) and by progressing insight created through other people's research that may reveal new patterns or associations in your data that may lead to further insights, claims and publications. Therefore, considering the much higher costs to offer the data yourself in a HPR or 'submit them to one of the emerging HPR environments is a worthwhile exercise. In many cases, private providers will be eager to include your data in HPR environments they provide (such as highly performant analytics graphs and visualisations), this may sometimes only pertain to part of your data, but it could expose your data in multiple HPR environments without you bearing the costs of the provision in those environments. The HPR provider makes a living by re-formatting, analysing, linking and integrating your data with OPEDAS and although the data as such as open and freely delivered to the HPR provider, an entire market on value added services around data is rapidly developing while you read.\n\nIn case you decide with the team that running your own HPR is not within your reach for technical, expertise, support or financial reasons, you may still consider to offer them formally for reuse in an existing HPR environment (Open Access or commercial). In other words consider the 'value' of your data up to the point where they may be an asset for commercial HPR providers in the cloud that may help you in publication (for free) in their desired format or even pay you for your data.\n\n## Do\n\n- Carefully consider the pro- and cons of HPR provision of your published (metadata)\n- Clearly distinguish between 'local' HPR environments (very costly) and joining an existing HPR provider where you let them offer your data in the desired format.\n- Discuss the 'value' of your data in the team, with your supervisors and make sure that due credit is given once your data actually does get reused and cited.\n\n## Don't\n\n- Lightly assume that just publishing your data (albeit FAIR) in an 'inert' repository is always just enough to claim good data stewardship. It certainly is from a minimalist standpoint, must it may severely limit your own chances to 'dig more gold from your data' later.\n- Think that offering your data in an HPR environment will jeopardize or lower your competitive value. Especially for big data sets, there is a chance that the participation in a broader set of frequently meta-analysed data will reveal new patterns (to others, in which case you should be cited, but also to you).\n"
    ]
  ,
    [ "jbz"
    , "7.3.1"
    , "# Where to publish?\n\n## What's up?\n\nData publishing is not the same as data archiving. The term 'publishing' has its roots in the notion of making assets (traditionally text or images) 'public'. No matter how much this term has undergone semantic drift in the current scholarly 'publishing' practice, we wish to stick to the original notion. So data archived in your local repository are not automatically 'published'. There may be a very good reason to keep them just 'archived' for internal reuse (obviously with all backup and safety issues discussed earlier, but here we assume that you want to really publish your data for external reuse. These costs should be eligible for the funder of your research and are comparable in nature to the APC for Open Access articles. However, it is not the responsibility of the creator of the data to keep the data in an HPR environment for many years for others to reuse. Reuse of OPEDAS is costly and part and parcel of proper budgeting for modern, data intensive research projects. So the actual reuse of OPEDAS in HPR formats should be eligible research costs in future grants. Open data are not free in terms of 'gratis' to reuse. This includes data and data infrastructure resources that are very intensively used. These should be recognized at some point as 'core infrastructure' and partly funded as a common good and/or from reuse fees.\n\nIn the total offering of open access publishers, obviously reliable reuse of the article for in principle an indefinite time is included. Storing huge data sets however, will be significantly more expensive (at least initially) than storing text. So long term preservation costs need to be taken into account. There is also a major difference between publishing (FAIR) data in a basic trusted repository where both people and machines will be able to find them, know under which conditions to access them (the F + A of FAIR) and between offering your data in fully Interoperable format, in a High Performance Reuse' (HPR) environment. Here we already warn you that HPR provision of your data to others is typically at least an order of magnitude more costly in terms of hardware and accompanying service and support than just 'publishing them' in a format that makes them principally FAIR. If your data are published, found and reused by others as OPEDAS, the new user will have to bear the costs to actually include the data in a re-analytics environment. So, publishing data is a different decision from maintaining (yourself) a HPR environment for them. Writing massive data to 'tape' and storing their rich and FAIR metadata in the 'cloud' will be usually still considered good data stewardship. The tapes can be found, accessed and the data can be retrieved in interoperable and thus re-usable format, may be at significant costs, but these should be borne by the re-user. The choice of your repository is extremely important though. Not only costs may differ significantly between the many emerging professional data publishers, but matters of trust and solid sustainability and support issues are minimally as important. In very general terms, professorware based local repositories are about the worst place to publish your data, but there will be as many 'data sharks' at the market soon as there are 'text sharks' today. So be aware of the strengths and weaknesses of various data publication options and don't take any beautiful web site at face value. The general attitude you may best adopt as a good data steward in open science is that money is not made on the data (as an asset with paid access0 but on services around the data, including HPR services, smart analytics etc.\n\n## Do\n\n- Publish data either internally or externally, but in all cases make sure you use reliable and sustainable repositories.\n- If published externally, make sure to use one of the certified trusted Data Repositories and trusted publishing groups.\n- Publish your (FAIR) metadata about the data set in a public repository, regardless of where the data themselves are stored.\n- Make a clear difference between 'inert publishing' and High Performance Reuse offering and budget for both separately\n- Make extremely clear as part of the FAIR metadata under which conditions the data can and may be reused (this goes way beyond a license and may for instance tell the user -frequently a machine- where to find the raw data and which steps are necessary to reload them in an HPR etc.)\n\n## Don't\n\n- Confuse archiving with publishing and 'inert publishing' with HPR offering.\n- Think that keeping data internal (even if for very good reasons like Patient privacy) relieves you from the good data stewardship practice to publish rich and FAIR metadata about the internal data set in a public and findable place, even if those metadata tell the potential re-user that reuse will be highly restricted.\n- Think too easily that your data have not been produced with public funding, and therefore you are not morally obliged to at least expose the metadata to the rest of the community.\n- Think that when you work in a private company all this does not apply to you. As long as private companies can deduce research activities as eligible tax exempt costs, the community is co-financing your research.\n"
    ]
  ,
    [ "cwq"
    , "7.2"
    , "# Who will pay for open access data publishing?\n\n## What's up?\n\nOpen Access publishing of articles is now eligible cost on most grants and increasingly, institutional policy is to support the publishing of articles in Open Access journals. However, the actual FAIR publishing of data is not yet as obvious. The example of the FANTOM5 paper in the introduction is a good demonstration of how data that 'are associated with' a paper can be an very important asset by itself, but will also cost significant time and effort to be published in FAIR format (in this case nanopublications). If we stick to the principle that data needs to be published in FAIR compliant formats to be optimally re-usable, separate planning for the capacity and the resources to do so is part of good data stewardship. Publishing an Open Access article with the actual data still being hardly Findable (hidden in supplementary data files on the deep web or without proper dedicated metadata), Accessible (in non-computer readable format and without a proper license) Interoperable (textual or non-community adopted identifiers) and thus very difficult to reuse should be considered bad research practice. Therefore the data on which a paper is based should be published in its own right and the 'supplementary article' (preferably Open Access) should be regarded as the description of the rationale, the methodology of the study that generated the data and the *first* set of conclusions and claims based on the paper. Returning to the FANTOM5 paper, the number of derivative papers based on the same dataset effectively shows the 'bankruptcy' of the classical publication method. The data set is key and all papers (including the first 'mothership' paper) could (should?) be seen as derivatives.\n\nNow the question arises how much of your data can be open access. As argued before, open science requires open access to be the default approach. However, there are situations where the data can not be made available in open access without restriction, even if the first and all subsequent derivative textual publications can be open access. For instance, data that are national security sensitive, or privacy sensitive such as patient data may have to be restricted in access. Obviously, this poses a fundamental issue already for peer review as the reviewers would have to 'trust' that the conclusions drawn in a particular derivative paper are based on solid data and represent credible conclusions. Therefore, even if raw data are to be restricted, a good data steward will always do everything needed and possible to provide a data set (anonymized for instance or aggregated) that allows the best possible review, reproduction and openness. In case the underlying raw data have to be restricted the provenance of the process that lead to the 'open data' needs to be as comprehensive as possible and the FAIR metadata of the restricted set should still be Open Access, including the Accessibility begin well explained (License, procedures to follow if access to the restricted data is to be requested etc.). However, data publishing may be many orders of magnitude more expensive than publshing narrative A small example: One modern instrument (for instance cryo-EM) easily produces around 1 terabytes (TB) per day. Thus, a typical high-end instrument research group that uses the available instrument time efficiently produces 100-300 TB per year. Typically, projects take 1-3 years, resulting in requirement of approximately 200-600 TB of storage for active projects. let's assume that the finder of the research requires that after completion, the data must be archived for at least 10 years. Thus, a data intensive research group may have to archive 2-6 petabytes of raw data over 10 years. Current (academic) prices are in the range of 50 Euros/TB/year + 180 Eu/TB/year for backup and for archiving 108 Eu/TB/year. So storing onepeta byte. So storing 1000 terabytes in a proper way might add up to well over 100.000 euros/year, just for archiving. This means that a data steward needs to budget very carefully for data storage archiving and back-up.\n\nA very important decision is: what part of the data to publish where and how. Storing metadata or annotations in a FAIR format and in a HPR environment is almost trivial from a cost perspective and may be enough to make the actual data optimally reused.\n\n## Do\n\n- Include resources in your study budget for the FAIR publishing of your data and/or metadata, regardless of whether they can be fully open access.\n- Be aware that publishing of large data sets may actually be much more expensive than publishing the accompanying article.\n- Work closely with experienced FAIR data colleagues and data publishers in- or outside your group to make sure that you indeed make the best (and affordable) technological, format and terminology choices that can be considered FAIR.\n- Choose a Trusted Data Repository to deposit the data once they are in FAIR publishable format.\n- Choose an appropriate license for the the data and include a machine readable identifier for that license in your metadata.\n- Make sure your data are citable.\n- Consider raw, processed, and metadata separately for publication choices.\n- Separate these data categories but ensure a resolvable permalink between them.\n\n## Don't\n\n- Publish (supplementary) data in PDF or any other non-machine readable format rendering them elusive for most text mining programs.\n- Assume that publishing the data coming from your experiment requires a budget that is in the margin, or even in the range of the APC of the articles you publish.\n- Publish any data without proper reference to the workflows and other research objects that were used to generate and analyse them.\n- Publish without a serious check on price, trust and quality of the provider.\n"
    ]
  ,
    [ "ivg"
    , "7.4.1"
    , "# What service to be offered around your data?\n\n## What's up?\n\nIn case you decide on more than 'inert publishing' or archiving in your own institute, you have to carefully manage expectations of your own research team and others on the level of services you provide on the data once published. Even if you do not choose for an internal HPR service, there may be significant costs involved if you ask an external provider to serve up your data in HPR status. This is a relatively new field so there might be regular changes and it may be wise to regularly check the emerging possibilities (see also external resources) to have an 'active' copy of your data in a HPR environment.\n\n## Do\n\n- Carefully consider the services you would like to attach to your data, both in case you decide on internal or external hosting.\n- Make sure that if you decide on 'active' data hosting, you have sufficient resources to support the HPR environment you choose.\n- Ensure that the services, analytics workflows etc. that you offer themselves answer to FAIR principles.\n- Include metadata on the 'status' of the data in term of their 'immediate' re-usability.\n- Include information about the possibility that your data can be accessed in 'inert' as well as in HPR environments and where.\n\n## Don't\n\n- Call an inert data repository an HPR environment.\n- Promise to people that they can 'reuse' your data without specifying the conditions, including the 'state' in which the data are offered (archived or HPR)\n- expect your (high quality as such) data to be intensively reused and cited if they are in a lousy repository or a lousy state of interoperability\n\n## Links\n\n- [DS Question GitHub resources repository: ivg](https://github.com/DSQResources/DSQ-ivg)\n"
    ]
  ,
    [ "jvm"
    , "7.1"
    , "# How much will be Open data/access?\n\n## What's up?\n\nOpen Access is a term frequently associated with 'science 1.0' when scholarly communication was still largely done via academic journals, later *extended* to the co-publication of 'supplementary data'. The actual text contained the new claims and the argumentation around them. The publishing system slowly became perverse as text was hidden behind firewalls (as researchers published 'for free') and then the reader should pay. Open Access as we know it is just another business model where the 'polluter pays', in other words, now the author pays a publication fee and the text is free to read for everyone. Much as this is an improvement, it does not fundamentally support Open Science better than the old system. First, authors in less fortunate institutions can not easily pay for publication and are again put in a disadvantaged position. Second, there was already far too much to read anyway, now there is even more to read. Third, data, the key asset for Open Science is not necessarily more Findable in Open Access papers then in closed papers (the TIFF walls and the broken links are not necessarily better). Therefore *publishing data in their own right* in Open Access is extremely important and a key task for a good data steward. We also need to clearly distinguish between metadata and the actual data here. It can be argued that metadata of any research object, regardless of whether it is an article, a data set or a piece of software should be FAIR and Open Access (the A in FAIR is distinct from Open as was discussed before). There might however be very good reasons to keep text or data under well defined, but restricted access. This may have routes in patient privacy or national security considerations, but (especially for data generated in the private sector) also reasons of competitive advantage. However, the latter is very difficult to argue for data that are generated with public funding. They should be considered a public good by default and arguments to keep them from reuse by others will have to be argued in any data stewardship plan with very strong arguments. Arguments to separate massive and crude data from FAIR metadata have also been discussed earlier. The raw data may be too bog to keep in a High Performance Reuse (HPR) environment (typically orders of magnitude more costly than for instance on tape). So in general, FAIR metadata make that your data and tools can participate in the FAIR open science ecosystem.\n\n## Do\n\n- Clearly consider for each data set whether it is considered generated by public or by private funding (or a mix)\n- Take as the default that metadata are always FAIR, with the A being Open Access without any restrictions.\n- Make a conscious decision with the research team for the best license to put on any given data set (the actual data elements.\n- Include the intended level of restriction in your data stewardship plan (even at the proposal stage). Again for most funders the default will be Open, so in case you want to argue for a more restrictive license, prepare your arguments extremely carefully as they may influence your chance to get the proposal awarded.\n- Work wherever possible with Open Source software, but never at the expense of aspects of professionalism, versioning, documentation, sustainability and (SLA) support.\n- When dealing with software: make your code open source under a well defined license, but consider sustainability issues before doing so. Open source code without proper community or licensed support is a 'poisoned gift' to the community as it may impair reproducibility of research in a serious way.\n\n## Don't\n\n- Consider data or software without a proper license 'open' en 're-usable'. It is not, companies will frequently not touch data or software until the licensing and support situation is clear.\n- Publish data or software on a license that is any more restrictive than what you really need.\n- Use restrictive licenses on data or software unless you really need to, for privacy, security or sustainability reasons, that will have to be argued hard core on future data stewardship plans.\n- Call data 'Open' and 're-usable' unless you have made sure they are also actually Findable outside your group, Accessible under well defined conditions and Interoperable for machines with only trivial adaptations. If any of those three elements is missing, they are still 'Re-useless'\n- publish data or software without the attributes and the instructions that make them properly citable.\n\n## Links\n\n- [DS Question GitHub resources repository: jvm](https://github.com/DSQResources/DSQ-jvm)\n"
    ]
  ,
    [ "mjf"
    , "7.3"
    , "# Where to publish?\n\n## What's up?\n\nData publishing is not the same as data archiving. The term 'publishing' has its roots in the notion of making assets (traditionally text or images) 'public'. No matter how much this term has undergone semantic drift in the current scholarly 'publishing' practice, we wish to stick to the original notion. So data archived in your local repository are not automatically 'published'. There may be a very good reason to keep them just 'archived' for internal reuse (obviously with all backup and safety issues discussed earlier, but here we assume that you want to really publish your data for external reuse. These costs should be eligible for the funder of your research and are comparable in nature to the APC for Open Access articles. However, it is not the responsibility of the creator of the data to keep the data in an HPR environment for many years for others to reuse. Reuse of OPEDAS is costly and part and parcel of proper budgeting for modern, data intensive research projects. So the actual reuse of OPEDAS in HPR formats should be eligible research costs in future grants. Open data are not free in terms of 'gratis' to reuse. This includes data and data infrastructure resources that are very intensively used. These should be recognized at some point as 'core infrastructure' and partly funded as a common good and/or from reuse fees.\n\nIn the total offering of open access publishers, obviously reliable reuse of the article for in principle an indefinite time is included. Storing huge data sets however, will be significantly more expensive (at least initially) than storing text. So long term preservation costs need to be taken into account. There is also a major difference between publishing (FAIR) data in a basic trusted repository where both people and machines will be able to find them, know under which conditions to access them (the F + A of FAIR) and between offering your data in fully Interoperable format, in a High Performance Reuse' (HPR) environment. Here we already warn you that HPR provision of your data to others is typically at least an order of magnitude more costly in terms of hardware and accompanying service and support than just 'publishing them' in a format that makes them principally FAIR. If your data are published, found and reused by others as OPEDAS, the new user will have to bear the costs to actually include the data in a re-analytics environment. So, publishing data is a different decision from maintaining (yourself) a HPR environment for them. Writing massive data to 'tape' and storing their rich and FAIR metadata in the 'cloud' will be usually still considered good data stewardship. The tapes can be found, accessed and the data can be retrieved in interoperable and thus re-usable format, may be at significant costs, but these should be borne by the re-user. The choice of your repository is extremely important though. Not only costs may differ significantly between the many emerging professional data publishers, but matters of trust and solid sustainability and support issues are minimally as important. In very general terms, professorware based local repositories are about the worst place to publish your data, but there will be as many 'data sharks' at the market soon as there are 'text sharks' today. So be aware of the strengths and weaknesses of various data publication options and don't take any beautiful web site at face value. The general attitude you may best adopt as a good data steward in open science is that money is not made on the data (as an asset with paid access0 but on services around the data, including HPR services, smart analytics etc.\n\n## Do\n\n- Publish data either internally or externally, but in all cases make sure you use reliable and sustainable repositories.\n- If published externally, make sure to use one of the certified trusted Data Repositories and trusted publishing groups.\n- Publish your (FAIR) metadata about the data set in a public repository, regardless of where the data themselves are stored.\n- Make a clear difference between 'inert publishing' and High Performance Reuse offering and budget for both separately\n- Make extremely clear as part of the FAIR metadata under which conditions the data can and may be reused (this goes way beyond a license and may for instance tell the user -frequently a machine- where to find the raw data and which steps are necessary to reload them in an HPR etc.)\n\n## Don't\n\n- Confuse archiving with publishing and 'inert publishing' with HPR offering.\n- Think that keeping data internal (even if for very good reasons like Patient privacy) relieves you from the good data stewardship practice to publish rich and FAIR metadata about the internal data set in a public and findable place, even if those metadata tell the potential re-user that reuse will be highly restricted.\n- Think too easily that your data have not been produced with public funding, and therefore you are not morally obliged to at least expose the metadata to the rest of the community.\n- Think that when you work in a private company all this does not apply to you. As long as private companies can deduce research activities as eligible tax exempt costs, the community is co-financing your research.\n"
    ]
  ,
    [ ""
    , "7.4.3"
    , "# Will you run your own access web service?\n\n## What's up?\n\nAlthough we recommend to use trusted external HPR services wherever possible, there may be reasons to run your own access service. For instance when data you generated cannot legally or technically leave you institutional firewall. In that case, you can only offer them for reuse by third parties by providing the data in a re-usable format and allow 'workflows ' (here used in the broadest possible sense) to visit the data. in that case you need to make sure that you can handle controlled access, authentication, authorisation, encryption, logging, monitoring of what goes in and out, as an internal service. This is far from trivial and will put significant challenges for your budget, infrastructure and internal expertise. In case it is unavoidable to keep data 'inside your firewall' there are serious management issues to be addressed like the training of support personnel and the acquisition (hardly ever the development) of standard software, hardware and services to allow all the data management access and processing steps mentioned above.\n\nYou also need to be fully aware of the various levels of guaranteed quality of service you want to offer, such as 'up time'; only at written request and ad hoc, academic best practice, 24/7 etc. and the budgetary consequences of such choices. For instance '24/7' may be an order of magnitude more complicated and costly than academic best practice access (the data is up if you are lucky, don't sue me if there is a problem that jeopardised your 3 day running workflow because my component was 'down'. The components you need to consider include (not exhaustive) : A processing service -including AAI, a download service, a hosting service, System, Hardware, Software and Financial administration and user logging. You also need to consider long term sustainability of the service; Who keeps data access running? and the instalment, staffing and trainmen for a help desk for data access and interpretation.\n\n## Do\n\n- carefully consider the reasons why data are not supposed to leave your firewalled institutional environment.\n- keep the amount of data offered for reuse for which this is indeed unavoidable to the bare minimum as they will cost you a lot.\n- Use as much as possible standardized, reliable and support public or private software, hardware and software providers to enable the services needed to make data available under active reuse conditions\n- (see resources, cloud providers,)\n\n## Don't\n\n- Develop any tools, algorithms or software needed to offer 'active' data under controlled conditions unless absolutely unavoidable.\n- Make sure that even if the data are of the most restricted category, the metadata is still made available in the most open format that your restrictions allow and include in the metadata rich information about the legal and technical needs and restrictions for the use of the actual data.\n- Describe your data as 're-usable in HPR if the management components are not reliable (i.e self-made) and your data component may therefore cause much trouble in pipelines or workflows of others because they did consider your web service as a reliable component of their work flow.\n"
    ]
  ,
    [ "igj"
    , "7.5.2"
    , "# Will you publish a narrative?\n\n## What's up?\n\nDisclaimer: the arguments put forward in this section are mostly relevant for hard core scientific communication, although any text (including this book) should benefit from taking many of the considerations into account. As said before, narrative articles (also called 'papers') will likely continue to exist for the foreseeable future as they still have their value. Computers can not easily cover rhetorical strings and arguments and therefore narrative (although inherently ambiguous and thus not FAIR) is needed to explain to people what was discerned in the data, how the data were generated and what is the basis of the conclusions. Part of all this, including methods etc. can be increasingly also captured in machine actionable format and therefore become part of the machine actionable FAIR ecosystem, but 'supplementary narrative text' with your data will always be needed and obviously review articles describing a whole field etc. will always play a role as long as human minds are part of the scientific process. However, when you write narrative, in whichever component of what you want to publish, take care to *never* introduce ambiguous symbols and formulations in the text. Narrative with synonyms and beautiful style figures may be more entertaining for people to read, but these do usually not at all serve the well-defined and straightforward scientific reasoning, but rather blur it. More importantly, they make text even more of a nightmare from machine reading, text and data mining than they already are by their very lingual nature. Unexplained acronyms (*people will know what I mean*) with multiple meanings (homonyms) alliterations, nested sentences and sentences of the style [concept 1, 2, 3,4 and 5 are all related to concept 6 in table 5] are *absolute malpractice* in text. In fact, a good exercise to internally check whether any narrative you add to your data is reasonably unambiguous, is to 'also' represent as much as you can of the text in a machine readable format and publish that along with the actual text. That means that if you use terms such as 'intellectual disability' and 'mental retardation' interchangeably or the same symbol for a gene and its corresponding protein, machines (and people alike) can at least check the unique identifier used in the machine friendly version in order to 'understand' what you actually mean. This exercise also forces you to structure your reasoning as precisely and logically as possible, so it will also improve the quality of your text. Also, refer to equipment, reagents and methods with unique identifiers linked to the term, and avoid using unspecific terms, jargon etc. Those sentences that cannot be represented in a machine readable format even after you tried probably represent exactly that (crucial) portion of your narrative that is needed to explain to other people 'how, why, and on what basis.\n\n## Do\n\n- Make a great effort to make any narrative you have to produce as straightforward and unambiguous as possible to make it more readable for both machines and people. Scientific text is not for leisure reading and entertainment, but should serve unambiguous scientific and scholarly communication wherever possible.\n- Only use narrative if you address people and not when your user is likely to be a machine and reduce the amount of narrative you produce to the minimum needed to communicate your methods, reasoning and rhetoric. We are bombarded with text to the extent that we need to read 70 hours a day to keep up with the 'literature' already.\n\n## Don't\n\n- Use undefined acronyms or synonyms unless unavoidable.\n- Produce one line more text than needed to effectively communicate your point.\n- Use text wherever machine readable and actionable formats can also do the job there will still be enough text left, don't worry)\n"
    ]
  ,
    [ "ezi"
    , "1.2"
    , "# Will you use Pre-existing data (including OPEDAS)?\n\n## What's up?\n\nEven if OPEDAS appear to exist, it is not automatic that you will use them. There might be considerations as discussed under 1.1., but it may also be just too cumbersome to get the data, or get them in a format that is easy to use for your particular study. However, if the answer is yes, there are a number of basic rules you need to stick to (after checking the features listed in 1.1)\n\n## Do\n\n- Consider whether you need 'all' the data some sets are very large and expensive to download and host- or just a relevant subset.\n- Determine whether you need to download the OPEDAS or whether you can 'use them where they are (for instance sending a process virtual machine or a workflow to the data).\n- Check the reuse (license) and citation/acknowledgement policy provided by the data owner.\n- If none is provided, contact the data owner to check these issues.\n- Make sure (with a second opinion) that using the data, even for small parts of your analysis, does not restrict you in publishing or using your results later on. If unclear, contact the data owner.\n- Actively annotate the data set used if there is any issue with it and submit these new metadata to a public, trusted repository.\n\n\n## Don't\n\n- Ever use OPEDAS without properly citing them in your resulting publications.\n- Use data without a proper license. Even if they seem entirely open.\n- Download unnecessarily large (portions of) data and host them locally.\n- Store data locally for longer than necessary (assuming the original repository is sustained)\n- Change anything in the downloaded data or its metadata without proper documentation and annotation.\n- Move into actual experimentation without consulting a [data expert]\n\n\n## Links\n\n- [DS Question GitHub resources repository: ezi](https://github.com/DSQResources/DSQ-ezi)\n"
    ]
  ,
    [ "fxe"
    , "7.4.4"
    , "# How and where will you be Archiving/Cataloging?\n\n## What's up?\n\nIt is obvious that only the 'active' version of any data set that is offered for direct reuse has to ever be in a HPR environment of any sort (and may be replicated in several of them for different purposes). The back-up(s) of the data set may be in a totally different format and accessibility status, for instance on tape.\n\nStill you need to be aware that in case the 'HPR version(s)' of the data gets lost or corrupted you are able to re-generate the HPR version of the data at short notice and without errors. It is crucially important that the location and the access procedures of the backup data that are archived is clearly stated in your institutional catalogue and not only understandable by you as the data steward. A good data steward will always consider the possibility that others will need to use any data or service (including the re-mounting of archived data) in situations where the data stewards is not available for consultation or help.\n\n## Do\n\n- Archive the backups of the data you provide for active reuse in the most stable and sustainable (also cheapest) way and format possible\n- Include the metadata and provenance of the archived (backup) data in the metadata files of the active data version\n- provide clear instructions for anyone in the institute who to find, process and remount the archived data into the format and the environments from which the original may have been lost or corrupted.\n- Regularly check the 'mounted' data in HPR for integrity and regenerate only when needed with full records and provenance of when, why and how the data were regenerated and remounted.\n\n## Don't\n\n- Assume that it is sufficient that you know where the archives are.\n- Do not consider a 'back up' as a simple copy of the entire HPR environment (considering that there is an order of magnitude effort and cost associated with the latter)\n- archive 'everything' but carefully consider what has to be archived as even archiving is very expensive if you enter the petabyte range.\n\n## Links\n\n- [DS Question GitHub resources repository: fxe](https://github.com/DSQResources/DSQ-fxe)\n"
    ]
  ,
    [ "atq"
    , "1.1"
    , "# Is there pre-existing data?\n\n## What's up?\n\nFor many decades if not centuries, virtually every experiment started with the collection or creation of 'observations' and in fact data. In social sciences and humanities the tendency to 'reuse' data that had been created earlier, in all kinds of surveys and increasingly of course from sources such social media maybe already somewhat more established. However, in many of the hard experimental sciences, the generation of new data specifically generated to answer a hypothetical question is still so commonplace that careful thinking about the actual need to generate new data may just not be on the radar screen. Obviously, data creation will need to continue, but increasingly we have to ask the question whether such new data are absolutely necessary to answer the question we want to answer. With more and more data becoming available in reusable format, there may well be existing data collections 'Other People's' Data and associated Services (OPEDAS) that without or with some extra effort needed, can answer at least part of the question or least may be crucial for the interpretation of your own data.\n\n## Do\n\n- Search for data sets (OPEDAS) that may be re-usable and can help you to reduce the number of new data sets you may have to generate (and steward later on).\n- Include annotated collections of data and curated databases in your search.\n- Check the accessibility and license situation attached to the relevant data sets you found.\n- Check their interoperability. They may be relevant but not interoperable with your analysis pipelines. In that case you may have to extract, transform and load (ETL) them or decide that -although relevant- they are not reusable for your purpose.\n- Ensure that using OPEDAS will not restrict in any way the use of your results later on, including copyright and freedom to operate on the request of IPR.\n- Check how to cite and acknowledge OPEDAS.\n- Consider to actively involved OPEDAS owners in your research in order to make optimal use of their data.\n- Speak to colleagues who did similar experiments before to find out about potential OPEDAS you may consider to use.\n\n## Don't\n\n- Assume no OPEDAS exist without thorough checking using all your possibilities.\n- Start an experiment without properly checking with colleagues about the best approach and OPEDAS out there.\n- budget for data generation in your study without justifying to the funder why the generation of the data is necessary.\n- Move into actual experimentation without consulting a data expert.\n\n## Links\n\n- [DS Question GitHub resources repository: atq](https://github.com/DSQResources/DSQ-atq)\n"
    ]
  ,
    [ "ckt"
    , "1.4"
    , "# Where is it available?\n\n## What's up?\n\nData sets (including reference data) may be available at different locations (in replica) and with different service level agreements attached. Established 'core' resources usually have a policy and a sustainability plan to ensure that the data will be available and properly versioned for a longer period of time. Smaller data sets and database may not have such crucial features secured, which poses a strong risk. Especially when you decide to use data resources outside of your immediate domain, this can be an easily overlooked issue. It is wise to use those resources that are most effectively accessible and sustainable. Data that are available in a repository that is not approved, does bring additional risks. If you use these data for your experiments and they are no longer available later on because for instance the repository went off line or got closed off, you may get into trouble for follow up experiments or reproducibility and review issues.\n\n## Do\n\n- Make sure you use data 'online' only when there is optimal insurance that the data will remain available (under the same conditions) 'indefinitely-in-principle'.\n- If you decide to use data from a non-authorized resource, make sure you download and keep the entire data file locally, with proper documentation on the provenance.\n- Check the performance issues related to using data 'online' versus 'locally'\n- Check all steps in intended workflows to be used in your data analysis down the line and whether they support the data formats and availability these workflows support.\n- make sure you have (access to) the capacity to develop ad hoc and custom workflows where existing ones may fail, given the intended analytics procedures.\n\n## Don't\n\n- Assume that all OPEDAS resources you use 'online' will automatically be there again (and in the same format) next time you need them\n- Use OPEDAS from a non-DSA resource if the same data are also available in a DSA version.\n- Use services that have no SLA or sustainability plan, as this will jeopardize the reproducibility of your research.\n\n## Links\n\n- [DS Question GitHub resources repository: ckt](https://github.com/DSQResources/DSQ-ckt)\n"
    ]
  ,
    [ "qej"
    , "1.15.3.2"
    , "# What Compute is needed?\n\n## What's up?\n\nOne of the most frequent mistakes is the assumption that sufficient compute will be available either locally or externally (service providers and cloud solutions). First of all, the ever expanding use of powerful high throughput technologies may generate data sets of a size and complexity that can not be properly handled by the local systems available to you (and you may have to book capacity). Storage is one thing you must have checked, but compute needed to process and analyse the data is yet another matter. Some data might be too large or too privacy sensitive to 'ship outside the firewall' of your institution. In that case the compute needs to go to the data inside your institution and sufficient compute needs to be locally available to run all processes needed. In many cases, when you come to the analysis phase you will need to use or download workflows and data from other places. Therefore compute capacity and expertise in your institution must be able to cover all these needs, and these need to be specified and checked before an expensive and complicated experiment is started.\n\n## Do\n\n- Make sure you know the ICT department that will have to 'deal with your data' if you do not control the hardware yourself.\n- Plan carefully with them how (and when) your data will be delivered to them and what processes are expected to be run.\n- Work with the compute specialists in your department with great respect and discuss needs and particularly also compute burden as well as (long-term) storage consequences of your data.\n\n## Don't\n\n- Ever underestimate the complexities related to professional data processing and the needed compute and storage infrastructure, skills and costs. If you find out later that you underestimated this, you will run into troubles, annoy your expert colleagues and you may even loose your data.\n- See the 'ICT department' as a necessary but less interesting and 'difficult' part of the institution and treat your colleagues as your 'data handlers' only, but engage them where possible in your research.\n- note: this is a typical place where 'local expertise and resources' as a 'button in the website' can be customised for the institution. Existing collections stewards can actively keep records here of what exists, thereby minimising the chance that naive PhD students and their equally naive (or overcommitted) PI's will unnecessarily start from scratch.\n"
    ]
  ,
    [ "mkg"
    , "1.15.4"
    , "# Will you create images?\n\n## What's up?\n\nImages pose particular challenges. Not only are images of modern instruments with high resolution and frequently sequentially collected very large in terms of storage, they do also contain a lot of 'intrinsic information' that makes them interpretable by people (and in some cases machines) probably even beyond your (current) imagination. Images are much easier to interpret 'in context' and therefore, if you need to anonymise or pseudonymised pictures (for instance medical images) make sure that you do not preclude that process during image creation or processing. The most important issue here is to separate annotations and metadata from the actual image.\n\n## Do\n\n- Consult colleagues on the need to be able to deliver the images to researchers (internal or external) without annotations and metadata embedded.\n- Consider this also when the images do not have privacy issues attached, there may be scientific reasons to preserve the ability to use and share the images without embedded annotations and metadata (for example for machine learning purposes).\n- Adorn pictures with rich and F.A.I.R. annotations and metadata (both intrinsic and user defined, see [xxx]) (see resources)\n\n## Don't\n\n- Embed annotations or metadata in the picture itself. This may seems logical as it will make them 'inseparable' from the image itself, but in actual fact it may severely restrict the reuse of the images for purposes other than your original intended use.\n- Store pictures without a very strong link to the associated metadata and annotations, but make sure you can always separate the two when needed.\n- Treat metadata and annotations of images with any less care than generically described for datasets\n- note: this is a typical place where 'local expertise and resources' as a 'button in the website' can be customised for the institution. Existing collections stewards can actively keep records here of what exists, thereby minimising the chance that naive PhD students and their equally naive (or overcommitted) PI's will unnecessarily start from scratch.\n"
    ]
  ,
    [ "acx"
    , "1.16"
    , "# Are there potential issues regarding data ownership and access control?\n\n## What's up?\n\nThere is a clear difference between legal data ownership (usually with the creator of the data or those who funded the experiments) and the degree to which the data will be made 'open' to and shared with others. Even entirely 'open' data have a legal owner. As insights in the extent to which data will be shared may change over time, you need to treat all data initially as 'private' to the creators. Opening up data is always possible, but keeping data restricted is not always possible anymore when the wrong choices have been made during creation, processing or publishing and licensing of the data.\n\n## Do\n\n- Consider data ownership and intended use, specifically also beyond the project in which the data were generated, as early on in the process as possible\n- Discuss ownership and control issues with the relevant experts in your institution (or external experts)\n- Choose a license for the data when they are ready to publish and consider reuse options (see also data citation and attribution [numbers]\n\n## Don't\n\n- Create data first and worry about ownership and access later. Features of the data may restrict choices later on in the process.\n- Ever consider data as 'just for this experiment' and thus underestimate reuse issues. Even if the data will be never reused in further integrated analyses, they still need to be always available (also to others like reviewers or auditors) for reproducibility checks.\n"
    ]
  ,
    [ "cvk"
    , "1.16.1"
    , "# Who needs access?\n\n## What's up?\n\nThere is a tendency to treat data as being for the sole purpose of the experiment for which they were generated. Even if that is the case there will usually be multiple people in the team (and not always only in your own institution that need regular access to them. Consider not only the legal, privacy and other 'social' issues around the data but also the technical hurdles that may arise when the data must be shared within the research group or consortium. This is not only related to the data themselves. If you have for instance used proprietary tools to preprocess the data or tools that are free for academic use but restricted for commercial use, it may not be allowed to share them with private, commercial partners in your research consortium. Also, if data cannot be moved due to size or sensitivity, make sure that all pearlers that will need them will be able to access them locally.\n\n## Do\n\n- Discuss intended data generation with all your consortium partners very early in the process\n- Ensure that licensed tools and for instance vocabularies that are used to process, map or format the data allow the sharing of the data in the entire consortium\n- Consider the size and the complexity of the data and anticipate any hurdles that these features may imply for the ease of sharing the data.\n\n## Don't\n\n- Assume that all Open Source tools and formats do allow all kinds of sharing.\n- Assume that all partners in the consortium only have academic and pre-competitive needs concerning the data\n- Assume that all partners have sufficient bandwidth and local storage and compute to deal with the data without checking.\n"
    ]
  ,
    [ "yqk"
    , "1.15.2"
    , "# What data formats do the instruments yield?\n\n## What's up?\n\nMost instruments in laboratories put out data in one or multiple formats that are specified by the manufacturer. In many cases, the downstream processes of data pre-processing, transformation, curation and linking will transform these data into other formats. It is very important to be fully aware of all data formats and standards that are 'machine' or 'instrument' imposed before the actual data capture and downstream processes begin.\n\n## Do\n\n- make sure you know all instruments to be exploited in the planned study and their imposed formats and limitations\n- make the data format that is the standard output of any instrument part of the provenance trail and metadata\n- record all relevant information about instruments\n\n## Don't\n\n- start measurements without the instrument landscape being fully clear, including constraints and limitations imposed by them\n- see the instrument part of the research as the sole responsibility of the experimental researcher and just passively wait for the data coming out.\n"
    ]
  ,
    [ "fqv"
    , "1.15.3"
    , "# What preprocessing is needed?\n\n## What's up?\n\nAs said before, in many cases you will process data in some way to prepare them for further analysis.\n\nThe preprocessing of data is sometimes standard and very easy, but it can also be almost a scientific challenge in and of itself, especially when instruments and approaches are relatively new.\n\nDeep understanding of the scientific goals, the context of the data, their structure and the underlying assumptions is needed to make the right choices concerning the processing. Here, we cannot go into detail about the hundreds of data/processing combinations that may need to be considered, but suffice to say: this is a matter of intense study with content and data specialists involved. Also, the provenance (logging, history) of precisely what happened to the data after their initial creation is absolutely critical.\n\n## Do\n\n- Ensure that full understanding is reached between content specialists and data specialists about the issues mentioned above before action is taken.\n- Carefully consider what the data represent, how they will be used downstream in your own experiments and potential later by others.\n- Log any processing you did to the raw data very precisely and in a F.A.I.R. format.\n- Make a permalink between the processed data files and whatever raw data may be preserved\n\n## Don't\n\n- Assume that you know how to do the processing and and that the processed data is all that matters. Increasingly, (data) publishers and funders will require the option to 'return to the raw data'\n- Store raw data without F.A.I.R. metadata and in a repository that is less trusted than where you store the preprocessed data. (note: these may be very different however, for instance you may need to store raw data on tape, while the processed data may have to function in high performance and re-analytics environments and be readily accessible to external workflow systems.\n- store useless raw data (good data stewardship also means deleting data that can not conceivably be of any further use).\n- Assume too easily that raw data are useless and fail to think about the long future, other users and other disciplines.\n"
    ]
  ,
    [ "eif"
    , "1.15.3.1"
    , "# Are there ready-to-use workflows?\n\n## What's up?\n\nFor many kinds of data there are established and well tested workflows and pipelines, turning raw data into analytics-ready formats. Make sure you are fully aware of these options for the kind of data at hand. Reproducibility of results and conclusions is strongly related to the standardisation of data formats, quality and the robustness (and version) of the workflows you use. The documentation and versioning of academic workflows (professorware) is not always up to standards and using the 'wrong version' of a workflow and its components later may give significantly different results on the same data used as a substrate. In some cases 'custom' analytics workflows are unavoidable, but if you have to use 'experimental' workflows, take extra care to document exactly what version you use.\n\n## Do\n\n- Study tool and workflow registries to make sure you use established and reproducible workflows wherever possible\n- If these do not exist and prototypic workflows need to be used, or when you have to develop custom workflows, make great effort to document code, versions of tools used (like the version of a vocabulary used for mapping)\n- Properly archive code, versions, components used and document as richly as possible\n- Add F.A.I.R. metadata to your workflows so that they could be Found, Accessed, Operated and Re-used by yourself and others with maximum garage of reproducibility.\n\n## Don't\n\n- Use custom made workflows unless absolutely unavoidable\n- Run these workflows without extensive documentation assuming that you will be the only one to process the data and only once.\n- Store workflow code and components on your local system without proper back up and metadata.\n- note: this is a typical place where 'local expertise and resources' as a 'button in the website' can be customised for the institution. Existing collections stewards can actively keep records here of what exists, thereby minimising the chance that naive PhD students and their equally naive (or overcommitted) PI's will unnecessarily start from scratch.\n"
    ]
  ,
    [ "ajz"
    , "1.16.2"
    , "# What level of data protection is needed?\n\n## What's up?\n\nWhenever data are generated, privacy and security issues need to be considered. This does not only hold for personal data on people. In many cases, the institution with have a policy about keeping data 'internal' and thus restricted in access. The reasons may be legitimate even in publicly funded research, where the default usually is make the data ultimately Open (or rather F.A.I.R.).\n\nIn many cases, data processing, quality control, curation and interpretation are lengthy processes and at least during these processes is can be counterproductive to give others than the data owners access to the data. They are simply not 'ready for reuse by others yet. So even if the data are intended to be made entirely public at the end of the pipeline, they may have to be restricted in access until the experimental evaluation is finished and the data are ready to 'publish'.\n\n## Do\n\n- Check with all investigators whether temporary or structural protection of the data is needed\n- Estimate the period over which the data needs to be protected and check the availability of infrastructure, funds and procedures to enable that.\n- Estimate the costs of long term protection of the data after the experiments have been concluded and the results published.\n\n## Don't\n\n- Assume that data (even if there are no privacy and sensitivity issues) can be just put on 'any laptop' in the institution. You might still be violating internal procedures\n- note: this is a typical place where 'local expertise and resources' as a 'button in the website' can be customised for the institution. Existing collections stewards can actively keep records here of what exists, thereby minimising the chance that naive PhD students and their equally naive (or overcommitted) PI's will unnecessarily start from scratch.\n"
    ]
  ,
    [ "zxp"
    , "1.16.2.1"
    , "# Is the collected data privacy sensitive?\n\n## What's up?\n\nIf data (for instance on patients' need to be structurally protected, you need to go through an in-depth evaluation of the levels of security needed, anonymization, pseudonymization and/or encryption of the data. You also need to make sure the consent obtained from the patient does allow all studies you want to perform on the data. An increasing challenge is that with increased reuse of valuable data sets and cohorts, reuse beyond the original consent is more and more frequent. Without renewed consent these further intended experiments may be illegal and therefore you need to obtain the broadest possible consent that is allowed by the ethical committees and acceptable for the individual patients. On option is to keep control of the personal data entirely in the hands of the research subjects, so that they can give informed consent for every future study that may want to involve their data. in that case special technologies are needed (see PHT). In all such cases, you need to be extra aware of possibilities to obtain the same results by re-using OPEDAS rather than unnecessarily create new privacy sensitive data that put significant responsibilities and financial challenges on your group.\n\n## Do\n\n- Check with all investigators whether temporary or structural protection of the data is needed\n- Estimate the period over which the data needs to be protected and check the availability of infrastructure, funds and procedures to enable that.\n- Estimate the costs of long term protection of the data after the experiments have been concluded and the results published.\n\n## Don't\n\n- Assume that data (even if there are no privacy and sensitivity issues) can be just put on 'any laptop' in the institution. You might still be violating internal procedures.\n- note: this is a typical place where 'local expertise and resources' as a 'button in the website' can be customised for the institution. Existing collections stewards can actively keep records here of what exists, thereby minimising the chance that naive PhD students and their equally naive (or overcommitted) PI's will unnecessarily start from scratch.\n"
    ]
  ,
    [ "jyd"
    , "1.11.2"
    , "# Will you need text mining?\n\n## What's up?\n\nText mining (recovering structured information from unstructured text) is a discipline in itself. You will find some literature here to read up on it if you need to. There was a time that major textual resources were not mined properly and individual researchers needed to recover concepts and their relations from text. However, nowadays, for many text corpora, there are well designed and state 0f the art collections of concepts, co-occurrences and relations mined by specialised consortia. Even these specialised groups will only roughly have the 80/80 level of precision and recall, but they are probably the best you are going to get. There may obviously be internal or obscure texts or textual collections that are not mined yet and therefore, if the information source is crucial to your research, you might consider to include text mining steps in your own protocols. However, again be aware that this is a very complex field and you may be ending up working on disambiguation, machine learning, thesauri, concept taggers and the like instead of doing your research.\n\n## Do\n\n- Double check whether the text corpus you are about to mine has not yet been mined and the results made available by others (see external resources)\n- Ensure that text mining is unavoidable (if the associations you hope to find are in public databases already as a result of previous text mining and/or manual curation, you will be most likely wasting most of your time to try and do better.\n- Consider to outsource the text mining (if really needed) to a specialised group or company. They are likely to get much better results than you will get with a 'homemade' mining algorithm.\n- Use 'ready to use' text mining software if you can not or will not outsource the task\n- Make sure that the mapping of terms to concepts in your output is correct and compatible with the data you want to combine the output with.\n- Budget for text mining in your research plan if it is unavoidable as a workflow step.\n\n## Don't\n\n- Underestimate the complexity of text mining as a method\n- Mine concept or relationships from any text without considering all other data stewardship issues as they are described here for any other data type (text mining results are highly about intensive and therefore potentially valuable data).\n- Develop new text mining algorithms unless your team agrees that none of the (many) existing systems are suitable for your purposes.\n"
    ]
  ,
    [ "ajm"
    , "1.11.3"
    , "# Do you need to integrate or link to a different type of data?\n\n## What's up?\n\nThe collection of OPEDAS sets you may want to exploit during different phases of your new study will partly determine how you go about setting up the actual experiments, and in particular also how you capture and format your own data. If the majority of the OPEDAS sets you need are in a given format, mapped to a particular set of term systems and can be processed by a given range of workflows, this may drive you towards generating your new data in a format and a semantic environment that is as closely as possible to the features of the relevant OPEDAS sets.\n\n## Do\n\n- Check what the predominant data formats are and make a list\n- Make a list of term systems used by the OPEDAS owners for concept-mapping\n- Discuss in the team how these lists might influence choices to be made during the next steps in the data cycle, in particular your data capture, term systems and standards used and formatting.\n- Check the compatibility of all datasets with the intended workflows to use for data analytics and determine the work needed to harmonise and reformat particular OPEDAS datasets for the analytics pipeline you anticipate to use.\n- Use all this input to determine in detail what is needed to enable you to start an integrated analysis of the OPEDAS sets (even if you do not generate new data this may still be a lot of work)\n- Make a detailed data re-formatting and capture plan geared towards the data analytics needs you have in mind.\n- Budget for that in your research plan.\n\n## Don't\n\n- Assume that workflows you choose for your analytics will consume and combine different data formats without any prior harmonization\n- Capture data first and then consider 2.1-2.11 to find out that you should have captured your data in a different format, with a different term system to map to and/or with different granularity.\n"
    ]
  ,
    [ "fqu"
    , "1.13.1"
    , "# Where will information about samples be stored?\n\n## What's up?\nLet's define two different kinds of data (in nature, not in format) about samples or other physical objects in a research collection that cannot themselves be made 'F.A.I.R.' in the sense of intrinsically findable, accessible, interoperable and reusable, including by machines. This holds true for 'tissue' or other physical samples, but also for instance for non-digital images like paintings, or a natural history collection of specimens, biological samples or you name it. These 'research objects' cannot be made F.A.I.R. themselves, but their 'descriptions' (annotations) starting with the legends on a video or a picture, growing to full annotations and 'metadata' on entire collections. Although one could argue that anything that is 'asserted' about a 'sample' is a form of metadata and is also considered an 'annotation', we will still make a distinction. We defined annotations in this context as a subset of 'user defined' metadata (see introduction) that give a functional description of the sample and its characteristics, what is was meant for, what it has been and can be used for etc. More classical 'metadata' are the 'intrinsic metadata about the sample itself (methods, timestamp, location etc.) and metadata about increasingly aggregated levels of the collection (up to the address of the building where the specimens are located).\n\n## Do\n\n- Apply all criteria for both annotations and intrinsic metadata as described for reference data in general (refer back to 1.12.) and register your collection in one of the existing catalogues (see resources)\n\n## Don't\n\n- create a biobank based on public money without proper F.A.I.R. annotations and metadata. Even for your own research this is crucial, but sharing your data as far as is legally allowed is the default and without F.A.I.R. metadata that will be seriously impaired.\n"
    ]
  ,
    [ "hhg"
    , "1.13.2"
    , "# Will your data and samples be added to an existing collection?\n\n## What's up?\n\nIt might be necessary to start an entirely new collection or biobank of research materials for your study, but in many cases it might be much more efficient and sustainable to add your samples and the associated annotation and metadata to existing environments. Reasons to start an entirely new collection might be that the samples and the data are so sensitive that they can not legally leave your institution and the institution does not yet have any 'biobanks'. In that case, you need to first think carefully if the collection will be sustainable in the first place. What happens to the data and the samples after your study is finished if no one else in your institution ever saw the need so far to create sustainable research collections in your institute? This will be a question reviewers will increasingly ask in data stewardship sections of research proposals. So, as argued before, first go through a careful process of finding out about existing biobanking initiatives in your institution or research consortium. If you decide that you have solid and defendable reasons to start an entirely new collection rather than adding a sub collection to an existing infrastructure, try to use generic criteria and infrastructure developed for professional biobanking in the relevant national and international consortia in this field (resources)\n\n## Do\n\n- Go through a very serious (and documented!) effort to search for opportunities to optimally use existing research sample collection infrastructure in your institution or consortium.\n- Make sure the existing or your new collection applies all criteria for both safeguarding or sample quality and preservation as well as the F.A.I.R. principles for all annotations and intrinsic metadata as described for reference data in general (refer back to 1.12.) and register your collection in one of the existing catalogues (see resources)\n\n## Don't\n\n- create an isolated and likely unsustainable biobank based on public money without proper infrastructure and without F.A.I.R. annotations and metadata. Even for your own research this is crucial, but sharing your data as far as is legally allowed is the default and without F.A.I.R. metadata that will be seriously impaired.\n"
    ]
  ,
    [ "csx"
    , "1.14"
    , "# Will you be collecting experimental data?\n\n## What's up?\n\nPlease note that it is no longer self-evident that answering a given research question needs 'de novo' experimental data. More and more examples appear in the literature [references] demonstrating that new discoveries can be made entirely based on 'OPEDAS'. As the generation of new experimental data is very expensive and time consuming. The rigorous questioning of the need to do so is part and parcel of good data stewardship for discovery. For a data driven scientist the optimal use of OPEDAS should be a default attitude. IN many cases however, you may need to create a (usually comparatively small) data set based on experiments, questionnaires or other observations. In that case it is obviously crucial that your data 'talk' to OPEDAS relevant for the interpretation of your new data, and that they can be processed by machines and workflows in conjunction with OPEDAS. Therefore, in addition to the usual statistical and 'methods' considerations when designing an experiment or a study, the way data are captured, formatted and published is a crucial D4D consideration and therefore data stewards should be regarded as essential expert colleagues already in the design phase of any study. Again, data stewardship is not just a service to others for whom your data will become OPEDAS, but the analysis and interpretation of the new data will be infinitely more easy, effective and reproducible when they 'feed into' standardized formats and workflows.\n\n## Do\n\n- Go through a very serious (and documented!) effort to search for opportunities to optimally use existing research data collection infrastructure in your institution or consortium.\n- Make sure your new data collection applies all criteria for both safeguarding or data quality and preservation as well as the F.A.I.R. principles for all data (if possible, see for example 1.13) but certainly for all annotations and intrinsic metadata as described for reference data in general (refer back to 1.12.) and register your collection in one of the existing data catalogues (see resources)\n\n## Don't\n\n- Assume that your data will never be used as OPEDAS or even reference data and therefore 'personalised data stewardship' (a metaphor for messing with your data in isolation) is acceptable.\n- Create an isolated and likely unsustainable dataset based on public money without proper infrastructure and without F.A.I.R. annotations and metadata. Even for your own research this is crucial, but sharing your data as far as is legally allowed is the default and without F.A.I.R. metadata that will be seriously impaired\n- Create more (or less) data than can be reasonably defined to be necessary for your study and without proper statistical and data analysis advice from the experts in your institution or (if not available) in your research consortium or elsewhere.\n\n## Links\n\n- [DS Question GitHub resources repository: csx](https://github.com/DSQResources/DSQ-csx)\n"
    ]
  ,
    [ "tgd"
    , "1.16.2.2"
    , "# Is your institutes' security sufficient for storage?\n\n## What's up?\n\nIf data (for instance on patients) need to be structurally protected, you need very specific infrastructure and security measures. If your institution would not have or support these (in house on in a private cloud type environment) you should seriously reconsider whether it is responsible to collect privacy sensitive data. Storing privacy sensitive data is a 'profession' not something you do on the side.\n\n## Do\n\n- Check with your ICT staff whether all requirements to store privacy sensitive data locally\n- Involve ethical and legal experts in the discussion on case of any doubt\n- If you intend to collect large datasets, also consider the costs and logistics related to size.\n\n## Don't\n\n- Assume that internal procedures developed by yourself will be sufficient\n- Assume that large datasets will be manageable by the current infrastructure of your institution. Expanding 24/7 reliable infrastructure with high security is a very costly issue and carries many specialised challenges.\n- note: this is a typical place where 'local expertise and resources' as a 'button in the website' can be customised for the institution. Existing collections stewards can actively keep records here of what exists, thereby minimising the chance that naive PhD students and their equally naive (or overcommitted) PI's will unnecessarily start from scratch.\n"
    ]
  ,
    [ "rzn"
    , "2.2.1"
    , "# Are there suitable terminology systems?\n\n## What's up?\n\nA key issue in science is to define any concept you refer to with the utmost possible precision. So in data stewardship and FAIR data a critical issue is the use of controlled vocabularies, thesauri and ontologies, here generally referred to as 'terminology systems', where the concepts you refer to are defined as precisely as possible. Before embarking on the mapping of the concepts in your data to any particular terminology system, you need to study the basics of these systems to at least know the differences between lists of controlled terms and synonyms and the actual use of ontologies where functional relationships between concepts are fixed. Ontologies assume a given abstraction of, and a particular view on, reality that may not always reflect the needs you have.\n\nControlled vocabularies and thesauri usually restrict themselves to providing terms with their defined meaning, synonyms (and in some cases homonyms, multiple meaning for the same symbol).\n\nFor deeper knowledge about terminology systems and specifically about the semiotic (Ogden triangle) and how this relates to data publishing we refer to reference *(Mons + Velterop)*\n\nIn brief: FAIR guiding principles assume that none of the concepts in your data is referred to with anything else than a machine-resolvable persistent identifier.\n\n## Do\n\n- Make sure you do not qualify yourself as a data expert without having a basic understanding of terminology systems, their strengths and weaknesses and their limitations.\n- Make sure that all concepts you want to refer to are covered by an existing terminology system with the highest possible community adoption.\n- Make a list of concepts present in your data and not covered by the first-choice terminology system.\n- Consider to use persistent identifiers from other terminology systems, following the same criteria.\n- Only if concepts are not properly identified in any existing terminology system and you do need to refer to them in your data, create a new concept identity and define its meaning.\n- Report the creation of a new concept + defined meaning to the nearest 'authority' you know and preferably to the stewards of the terminology system that would be the most likely home for the newly defined concept.\n\n## Don't\n\n- Mix the terms 'ontology', 'controlled vocabulary' and 'thesaurus' indiscriminately in conversations and text.\n- Use ontologies to refer to if you can as well just refer to a controlled vocabulary. Ontologies assume many links of the concept you refer to with other concepts, which may or may not be correct in your case.\n- Create a new symbol or identifier for a concept in your data without an exhaustive search for existing terminology systems.\n- Create a concept without a first attempt to define what 'unit of thought' you exactly mean by the new identifier.\n- Create your own sub-terminology system without sharing that effort with the community."
    ]
  ,
    [ "kjp"
    , "2.5.1.1"
    , "# Will you be archiving data for long term preservation?\n\n## What's up?\n\nLong term archiving and preservation is clearly distinct from 'short term (re-) use of the data for the experimental procedures immediately following its generation. Even for short term use, data need to be stored and backed-up in different places with the appropriate safety and access considerations.\n\nHowever, when it is decided to keep the (reusable) data for prolonged time periods, the scale of these issues increases. The conditions under which others can use the data later on may differ (legally or practically) from the way in which the data was used for its original purpose. Therefore, access, constant availability (or not) and many other issues may have to be considered. It is good practice to anticipate the issues associated with the long term preservation and reuse up front and budget for them as part and parcel of the data steward plan when the data generating experiments are designed and planned.\n\n## Do\n\n- consider data 'long term re-usable unless ...' It is easy to assume that data (small or less) are only of interest to the person or the small group that generates them. However, as even if you think (correctly) that the data will be never useful for any other experiments, the very minimum requirement is that they remain available for others (including reviewers requesting re-running of experimental or analytical workflows.\n- Always keep the persistent identifier of the data set available. Even if the data is deemed to be of no use any longer, and they are taken off line or even destroyed, the fact that the data set was there (and may have been cited by others) needs to be traceable.\n- Continue to update the metadata of the data set to ensure that its fate (including reuse) over time is always traceable. It is highly recommended to create a 'explanatory file' describing what the data was and why it was taken off line when the decision is made to no longer preserve the data.\n\n## Don't\n\n- Assume that long term preservation is something to 'worry about later' and upset the ICT colleagues with *ad hoc* and urgent solutions. when it appears the data are 'valuable after all'\n- Store ANY data set (small or large) without obtaining a registered. PID (for instance a DOI) for the dataset.\n- Wait with this until the data are preserved, but make this a routine at data generation time.\n- Let data change over time (even just location) without making sure it remains Findable and Accessible.\n\n## Links\n\n- [DS Question GitHub resources repository: kjp](https://github.com/DSQResources/DSQ-kjp)\n"
    ]
  ,
    [ "rht"
    , "2.5.2"
    , "# When is the data archived?\n\n## What's up?\n\nThe term 'archived' is usually reserved for the process when data 'retires' from the project for which is was originally generated. However, in the data driven science era, such data may be 'called from retirement' at any point in time. Still, we keep the term 'archiving' here for the process that follows after the intensive use period in the data generating project. Archiving for preservation only (let's call it 'taping') may render the data integer and recoverable but not necessarily immediately re-usable\n\nWe should still consider these data part of the FAIR ecosystem in case the FAIR metadata are still exposed and both people and machines are able to 'Find them', find out what their Accessibility level is (needs to be recovered from tape, needs personal contact etc.) even if it will take considerable effort to reconstruct the data in readily Interoperable and Reusable format. It should be decided as early as possible in the process how and when the data will be archived, how, in what format on what carrier and how they will be protected against calamities, unwanted or improper use and theft.\n\n## Do\n\n- Decide with the team what the best time point and method is to archive the data for long term preservation.\n- Keep rich documentation of the procedures followed to format the data for long term archiving and consider potential sources of error and change introduced but these procedures.\n- Record the exact time of archival and the authority (repository?) that takes over responsibility.\n- Give that archived version of the data a new PID (and if you destroy the 'working version' of the data, keep that PID for later reference.\n\n## Don't\n\n- Mix archiving with keeping data in store for reuse in the same study cycle or for further processing.\n- Mix up archiving with keeping data in 'active state'.\n- consider data that are 'somewhere on a disc' as properly archived\n"
    ]
  ,
    [ "eky"
    , "2.5.1.3.1"
    , "# Are you using backups for restoring files that were accidentally deleted or changed?\n\n## What's up?\n\nEven if changes (including unintended loss) of parts of data spark 'restoration' from backup files, especially if those were not under your own control and you can not guarantee that the backup was still fully identical to the original, you need to record and document that procedure. If people will reuse the restored data under the assumption they were using the original and they find unexplainable differences in their results, they need to be able to trace this back to the (potential) errors that were introduced during the restoration process from external back ups.\n\n## Do\n\n- Record and document everything that happened to the original as well as the back ups.\n- Require the internal and external parties that take care of your backups to inform you immediately if there is any reason to assume that the backup does not 100% resemble the original anymore.\n- This could include backups of identical subsets of for example toxicity samples and tissue samples. (accidentally thawed?)\n\n## Don't\n\n- Think that back up only starts when data is archived, you may lose them during experimentation.\n- Assume that back-up files of archived, reformatted or even destroyed data and their metadata deserve any less attention and care than those of active data.\n"
    ]
  ,
    [ "yqy"
    , "2.5.1"
    , "# Storage Capacity Planning\n\n## What's up?\n\nStorage issues do not necessarily scale linearly with the size of the data in 'bytes'. The complexity and the nature of the data should be taken into consideration and at certain sizes there might be a sudden 'breaking point', for instance your internally available infrastructure cannot handle it anymore, or there is an institutional policy concerning maximum data sizes.\n\n## Do\n\n- Check the availability of sufficient and reliable storage capacity in your institute and discuss with the department responsible for it.\n- Make an upfront calculation of the full costs of the initial storage, backup, and long term preservation of the data with the experts in the group.\n- Inform the local ICT experts about the intended use and reuse of the data over time and discuss the consequences of that plan for the sort of storage and availability needed. This will largely determine the costs.\n- Consider all options to reduce data sizes without information loss and smart sharing options.\n\n## Don't\n\n- Assume that the storage in your institute, even if it is very large, is 'just there'. Storage is no longer something that simply will become cheaper so fast that it can be considered marginal cost.\n- Surprise your institutional ICT staff with large data sets they are 'supposed to store' as it is their task.\n- Consider storage as pure 'archival', but always consider the use and reuse requirements at the same time as they may significantly influence the choices made (for instance can data be stored on tape or not?)\n"
    ]
  ,
    [ "hfg"
    , "3.2.1.1"
    , "# Case report forms?\n\n## What's up?\n\nCase reports from a special kind of data source. Case reports are frequent in medical, but also in biodiversity and environmental studies, but they can go as far as case reports of law enforcement officers in daily practice. These case reports are not necessarily always seen by the creator as a 'research object' but more often as a routine record for internal reporting purposes. However, many of those become research objects at a later stage. In many cases they will be collected by researchers who hope to find patterns, trends and evidence in such case reports for many different reasons.\n\nIn many cases therefore these data sources come as they are and you have to deal with them as *intrinsically suboptimal* raw data. Text mining, human interpretation by others than the creators of the text or structured data file can introduce further sources of variance. It is therefore important to carefully prepare the team for such sources of variance. Although some of them are simply unavoidable, maximum care should be taken that wherever this variance can be prevented it should be. For example, the team should make a clear strategy how to deal with ambiguous language in case reports and preferably map everything to the standard vocabulary before analyses are undertaken. Where possible, feed back loops with the original creators of the case reports should be attempted when ambiguities might influence the results. Variance in human interpretation in the team should be studied and recorded, precision and recall or text mining programmes run to extract structured information from text should be described and be part of the metadata and any such sources of variance should be part and parcel of the metadata and process descriptions. Erroneous mapping to concept PIDs by text mining tools is a very frequent source of errors and should be carefully studied and recorded whenever text mining is part of the analysis pipeline.\n\n## Do\n\n- Make the research team fully aware of potential sources of variance and errors in the *post-hoc* interpretation of structured and unstructured sources for research such as case reports\n- Make the recording and study of non-controllable human behaviour and interpretation a point of study and recording in the broader project.\n- Make clear plans with the team about how to correct, normalise and account for the known sources of variation in such *post hoc* studies\n\n## Don't\n\n- Treat *post hoc* research objects (not originally being created with a particular study in mind) as equal to research objects, data and data sources that were purposely and carefully created as data sources for research.\n- Ever assume that two people will interpret the same unstructured data source (such as a text) in the same way.\n- Underestimate the enormous ambiguity that human-created text contains. Humans are trained to use various synonyms for the same concept (assuming to keep the text more 'readable' and 'interesting' for others), are inclined to use jargon, acronyms and other ambiguous symbols to refer to concepts that are 'obvious' to them in context, but not at all unambiguous for later readers, let alone for machines.\n- Assume that machine interpretation of free text after mining and natural language processing or of images and audio files is anywhere near flawless. Lot's of controls are needed to reduce the errors induced by machine interpretation of research objects originally meant for humans.\n\n## Links\n\n- [DS Question GitHub resources repository: hfg](https://github.com/DSQResources/DSQ-hfg)\n"
    ]
  ,
    [ "bpp"
    , "2.5.7"
    , "# How frequently will you archive data?\n\n## What's up?\n\nThe decision to archive data for later reuse (by yourself or others) depends on a lot of variables. It is first of all important to understand and agree on how frequently and for what exact purpose the project partners will need to access the workspace where the data are residing. Some data may need to be mounted for immediate use all the time or remote-mounting may be needed. As a remote mount uses Network File System (NFS) to connect to directories on other machines so that they can be used as if they were all part of the user's file system, this option may only be viable for relatively small data sets. There may be many steps of intermediate data that need to be considered, need to be stored for later reference, review and reproducibility checks, but these need not be necessarily all mounted all the time. Obviously, when you copy data to local work spaces they may or may not need the same level of performance, security and networks speed as the main workspace. Also, if there is no 'local expertise' at the sub workspace, you need to plan for support.\n\n## Do\n\n- Discuss a long-term data sustainability and reuse plan with the team for each data set generated, downloaded or acquired.\n- Plan for a budget and regular 'expiration options' meetings for all data sets under your stewardship.\n- Transfer these plans, resources and responsibilities explicitly to the trust third party you may give stewardship over your data.\n- Make sure that even if data are 'destroyed' or 'tape-archived', the metadata as well as the unique identifier always stay FAIR.\n\n## Don't\n\n- Mix re-usability with actual reuse: there are many examples where intrinsically valuable data had not been used for many years and suddenly appeared to be crucial. Similarly there are examples of data that have been lost and would now be very valuable.\n- Ever throw away data, metadata, code or tools without informing the team and discussing the decisions. It is not always easy for the data steward to determine the actual expiration date of data.\n- keep data (even small sets) when there is clear evidence that they are corrupted, false, wrong or obsolete for other reasons, even if there is no 'storage' or financial reason to delete them.\n"
    ]
  ,
    [ "cbq"
    , "2.6"
    , "# Is there (critical) software in the workspace?\n\n## What's up?\n\nAs argued before, the separation between data and 'software' (executable code) is close to blurred in data driven science. In many cases therefore, your experimental workspace will hold you *de novo* research data, OPEDAS and software packages that you use to process and analyse the data.\n\nThis means that 'active data' like executable code may influence the workspace in a different way from 'static' data. Software packages that are Open Source and not properly supported my seriously disrupt the workspace and cause all kinds of troubles. Also, software used in the workspace may carry with it certain licenses that render the processed data unusable for the purposes you had in mind with them. First of all, it is therefore critically important that you never mix the loaded data for processing and analysis as they are mounted in the active workspace counting as 'one of the copies' or even a back up. The principle attitude must be that data that are actively used in the workspace will at some point be corrupted or lost due to unforeseen events. But also software itself (when active) may become corrupted and may need to be restored from another source in order to re-run processes and analytics.\n\n## Do\n\n- Keep 'static' and safe back ups of both data and software that is mounted in the workspace\n- Conduct regular integrity checks on the data elements in the workspace to prevent unexpected outcomes due to corruption of data, software or processes. unnoticed events may seriously slow down proper experimentation, analytics and interpretation of data and you as a steward cannot expect the rest of the research team to notice.\n- Make sure that versions of the software and workflows (including plug-ins like vocabulary services) do not change unexpectedly (academics are notorious in changing things without notifying potential external users).\n- Treat your workspace as a 'stand alone' and time delimited environment where you run stable data and processes (so for instance no 'remote mounting of thesauri of web services you do not control).\n- Keep very careful records of versions and potential issues of all elements (data as well as software) that were used to run a particular process at a particular time. Basically treat each 'run' of a workflow system as a 'batch' with a unique identifier and rich provenance.\n- Make sure you can always restore safely and quickly, an 'identical' workspace (although always a new 'batch' identifier will be needed) from the archive.\n\n## Don't\n\n- Ever regard the active workspace, its data and its software as 'another backup' of your data.\n- Produce and deliver data from the workspace back to the team without carefully monitored and described 'provenance' and a 'batch' identifier.\n\n## Links\n\n- [DS Question GitHub resources repository: cbq](https://github.com/DSQResources/DSQ-cbq)\n"
    ]
  ,
    [ "ybw"
    , "3.2.1"
    , "# Do you have non-equipment data capture? (questionnaires, free text etc.)\n\n## What's up?\n\nThere are a lot of data capture procedures that are not strictly spoken 'instrumental output'. It is obvious that you can not walk with the researcher in the field that takes water or soil samples, be at the table of the pathologist that takes a tissue sample or the microscopist that makes sections, stains, embeds and preserves them, nor can you walk with each research assistant approaching people with questionnaires. However, careful consideration of how data capture procedures that have a strong non-standardised (human) component such as active questioning of subjects or surgical procedures may influence the ultimate data you will have to steward is a key task for a good data steward. Very much like machines may need to be calibrated at regular times and data may slightly vary based on how long ago that calibration took place, different students may cause different biases in the filing of questionnaires based on subtle signals they send to the interviewed person. Obviously, as with machines, the data steward cannot change that, but for instance, careful recording of who took what survey when and under which conditions is extremely important.\n\n## Do\n\n- 'Know your source': make sure that when data are generated in 'variable circumstances by non-standardizable agents (such as humans) you are aware of that and you have procedures in place to detect unintendedly introduced variance (and correct for it if possible).\n- Discuss data-view based concerns with the research team and make sure that all data and sample collectors are optimally aware of the value of standard operating procedures but also of the need to record any abnormality encountered during the data capture procedure.\n- Keep all records of the capture and pre-processing procedures even after data have been harmonized and normalized. This is not only for review and reproducibility purposes, but also to ensure that if unexpected trends are observed.\n\n## Don't\n\n- Consider the data or sample capture people and procedures 'none of your business'\n- leave the considerations about variables in data capture procedures to the experimental team and (again) see yourself merely as the 'recipient of the data' when they come out of the experimental workflow.\n- Consider any recorded data and metadata during the capture process as disposable at any later stage unless there is a very good argument to destroy them.\n\n## Links\n\n- [DS Question GitHub resources repository: ybw](https://github.com/DSQResources/DSQ-ybw)\n"
    ]
  ,
    [ "ydj"
    , "4.2"
    , "# Choose the workflow engine\n\n## What's up?\n\nIn many cases end-to-end pipelines may exist for the type of data processing you need. However, it may also be true that you need only particular components of a workflow. When workflows are based on serialised components (for instance web services) that can be relatively independently deployed, it may be an option to use only part of an existing workflow. However, be aware of the stability issues related to web services. In case web services are maintained by academic groups, there may very well be issues related to proper documentation, versioning and support that can severely hamper your experimental procedures. If the reliability of the average web service is 80% (up-time, versioning etc.) and you need a sequence of 5 web services to run your workflow, do the math on how likely it is that the entire workflow will run smoothly. Therefore, if professional tools exist (even if they are proprietary and not Open Source) carefully study and discuss the balance between using Open Source software that you can adapt yourself, its professional support level (which may cost you regardless of the license situation of the software) and the use of commercial software (Open Source or Closed). There is obviously a tendency in academia to use Open Source software whenever possible, but this is not always adding to efficiency, just to 'perceived freedom to hack' and also sometimes based on a strong bias against 'commercial software' and the fear for 'vendor lock in'. These fears are not always unjustified but going for unprofessional 'hacks' just to save some money (in the short term) or based on your desire to be able to 'change and tweak' things yourself (or even worse, purely motivated by the desire to publish on something new *per se*) may not at all be the best servant for your research team. Laboratory analysts would be punished immediately if they crafted their test tubes themselves or made their reagents in a non GLP environment, so why would that be acceptable for data analysts? So the choice of workflow engine is as much a team choice (with your advise) as the choice to use reagents such as antibodies from a commercial (guaranteed) provider versus producing them in the lab by immunising mice. Questions to be answered in the process of choosing an existing workflow, engine or service include considerations about the ease of custom developments. For instance; can a workflow be edited collaboratively, can you reach out to and collaborate with the developers? Does it support the compute back ends you need?, does it offer standard tools for the administrative operations? What is the ease of adding new tools, does it support nesting of workflows? is it easy to use, also for non-computer experts, for instance does it have a running and easy GUI?\n\n## Do\n\n- Check the various workflow options and engines, not only for promised functionality but also for:\n- Level of documentation, support, licensing and reliability.\n- Keep in mind that your final responsibility is the quality of the processed data, not to make new algorithms.\n- Get in direct contact with the workflow engine provider and follow training if necessary.\n- Run the workflow, engine, pipeline on exemplar data first.\n- Take the 'running' environment into account and check the sustainability of that environment (for instance Galaxy)\n- check who in the team will have to operate the workflow (experimentalists and ICT people may prefern very different UI's)\n\n## Don't\n\n- Develop new workflows or workflow engines unless demonstrably unavoidable and consented by the team.\n- Use unsupported tools or environments 'just because they are easy to access and OS'.\n- Discard the idea to use commercial or otherwise proprietary workflows without careful consideration and discussion with the rest of the team, especially when your data are difficult to reproduce.\n- Expect purely academically published and 'provided' workflows and web services to be 'up and running' all the time and consider an entire workflow as one unit, without inspecting all serial elements of it for reliability and usefulness.\n\n## Links\n\n- [DS Question GitHub resources repository: ydj](https://github.com/DSQResources/DSQ-ydj)\n"
    ]
  ,
    [ "dwv"
    , "4.3"
    , "# Workflow running\n\n## What's up?\n\nA workflow run without supervision is a grave risk to your project. When you run a workflow, you need to have a system in place to monitor progress and potential errors. Not all workflows and services have professional and built-in error messaging, restore possibilities etc. It is a disaster if you only find out that the workflow has been stalled (a web service component was off-line for instance) after a long time and after potentially irreparable damage to the data has been done. Obviously, always also keep a backup of the original data that were entered into the workflow as some workflows may discard interim files, which may make rescue of data immediately after workflow-internal error only possible by starting all over again with a repaired workflow and the original data.\n\n## Do\n\n- Monitor every workflow that is running on a continuous basis (or install/develop machine-monitoring'\n- Keep a copy of the original input to enable restarting of the workflow at any time with the original input.\n- Make sure you record every step of the workflow (completed successfully, error detected, component restarted etc.) in order to be able to trace the source of aberrations in outcome.\n- If the input of the workflow is subject to change over time (for instance sample measurement where quality of sample or staining/labelling is subject to decay) take extra measure (run the supervised workflow with reference data before starting the real experiment.\n\n## Don't\n\n- Let any workflow run completely unsupervised and assume the output is correct and can be fed into the next step of processing or analysis.\n- Restart a workflow after a detected error without carefully recording what the error was and what was done to repair the process.\n- Re-run workflows that work on input that may subject to change over time (and will therefore give different results when re-run even if the workflow did a perfect job).\n"
    ]
  ,
    [ "egv"
    , "4.2.4"
    , "# Verify repeatedly on the same data\n\n## What's up?\n\nWorkflows, especially composite ones are inherently unstable unless otherwise proven. One way to verify that a workflow is stable, especially if it is composed of multiple independently developed and operating components, is to run it on a regular basis on standard reference and calibration data sets. If results on the reference calibration set are different, apparently one of the components of the workflow is not functioning properly or has been changed. The change may actually be an improvement from the point of the continent provider, but in many cases, updates and new versions are released without proper pre-notification to all users (if these are known and tracked in the first place).\n\nSo, even if components of a third party workflow are improved they may still cause irreparable damage to your research by introducing unnoticed or otherwise irreparable aberrations or even damage to data. So especially when a remote workflow or service has not been used for a while in your local setting you must carefully check and verify that the results of the workflow on your reference data set are identical to the last time you ran the workflow.\n\n## Do\n\n- Maintain stable reference and calibration data sets for each workflow operational in your research setting.\n- Run these at a regular basis in order to prevent that you only find out that the workflow has been changed at the moment you need it urgently and your raw data or measurements may suffer.\n- Contact all providers of workflows and make sure they know you are using their tools and ask for notification of changes or errors.\n\n## Don't\n\n- Ever re-run a workflow on new data (especially when the results 'add to accumulating evidence' just assuming that the workflow (even if claimed to be stable) is indeed stable.\n- Accumulate evidence and data in collections that were generated at different time points with potentially changing workflows that have corrupted or otherwise rendered subsets of your collection 'outliers' that may completely mess up the collection.\n\n## Links\n\n- [DS Question GitHub resources repository: egv](https://github.com/DSQResources/DSQ-egv)\n"
    ]
  ]
